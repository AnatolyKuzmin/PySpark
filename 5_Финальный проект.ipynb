{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504737b7-406a-444f-920e-ab624bec4cb2",
   "metadata": {},
   "source": [
    "## **1. Постановка задачи**\n",
    "**Цель**: Построить систему предсказания оттока клиентов телеком-компании.\n",
    "\n",
    "**Данные**: \n",
    "- Исторические данные о клиентах (CSV, 50+ млн строк)\n",
    "- Логи взаимодействий (JSON, 1+ TB в S3)\n",
    "- Внешние данные о регионах (Parquet)\n",
    "\n",
    "**Требования**:\n",
    "1. Очистка и объединение данных из разных источников\n",
    "2. Feature engineering\n",
    "3. Обучение модели машинного обучения\n",
    "4. Визуализация результатов\n",
    "5. Оптимизация производительности\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Архитектура решения**\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Raw Data] --> B[ETL Pipeline]\n",
    "    B --> C[Feature Store]\n",
    "    C --> D[ML Training]\n",
    "    D --> E[Model Serving]\n",
    "    E --> F[Dashboard]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Реализация проекта**\n",
    "\n",
    "### **3.1. Инициализация Spark-сессии с оптимизациями**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomerChurnPrediction\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### **3.2. ETL-пайплайн**\n",
    "```python\n",
    "# 1. Загрузка данных\n",
    "customers = spark.read.csv(\"s3://data-lake/customers/*.csv\", header=True)\n",
    "interactions = spark.read.json(\"s3://data-lake/interactions/*.json\")\n",
    "regions = spark.read.parquet(\"s3://data-lake/regions/\")\n",
    "\n",
    "# 2. Очистка данных\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "clean_customers = customers.withColumn(\n",
    "    \"churn\",\n",
    "    when(col(\"status\") == \"cancelled\", 1).otherwise(0)\n",
    ").drop(\"status\")\n",
    "\n",
    "# 3. Обогащение данных\n",
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "enriched = clean_customers.join(regions, \"region_id\") \\\n",
    "    .withColumn(\"tenure_days\", datediff(current_date(), col(\"join_date\")))\n",
    "```\n",
    "\n",
    "### **3.3. Feature Engineering**\n",
    "```python\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler,\n",
    "    StringIndexer,\n",
    "    OneHotEncoder\n",
    ")\n",
    "\n",
    "# Категориальные признаки\n",
    "cat_cols = [\"plan_type\", \"device_type\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_enc\") for c in cat_cols]\n",
    "\n",
    "# Числовые признаки\n",
    "num_cols = [\"tenure_days\", \"monthly_charges\", \"total_charges\"]\n",
    "assembler = VectorAssembler(inputCols=num_cols + [f\"{c}_enc\" for c in cat_cols], outputCol=\"features\")\n",
    "\n",
    "# Пайплайн преобразований\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "model = pipeline.fit(enriched)\n",
    "features = model.transform(enriched)\n",
    "```\n",
    "\n",
    "### **3.4. Обучение модели**\n",
    "```python\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Разделение данных\n",
    "train, test = features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Инициализация модели\n",
    "gbt = GBTClassifier(labelCol=\"churn\", featuresCol=\"features\")\n",
    "\n",
    "# Подбор гиперпараметров\n",
    "param_grid = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxDepth, [3, 5])\n",
    "    .addGrid(gbt.maxIter, [50, 100])\n",
    "    .build())\n",
    "\n",
    "# Кросс-валидация\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"churn\")\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Обучение\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "# Оценка\n",
    "predictions = cv_model.transform(test)\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC-ROC: {auc:.3f}\")\n",
    "```\n",
    "\n",
    "### **3.5. Визуализация результатов**\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Конвертация в Pandas для визуализации\n",
    "pd_results = predictions.select(\"churn\", \"prediction\").toPandas()\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(pd_results[\"churn\"], pd_results[\"prediction\"])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(f\"ROC Curve (AUC = {auc:.3f})\")\n",
    "plt.savefig(\"roc_curve.png\")\n",
    "```\n",
    "\n",
    "### **3.6. Деплой модели**\n",
    "```python\n",
    "# Сохранение пайплайна и модели\n",
    "cv_model.write().overwrite().save(\"s3://model-bucket/churn_model\")\n",
    "\n",
    "# Загрузка для предсказаний\n",
    "from pyspark.ml import PipelineModel\n",
    "loaded_model = PipelineModel.load(\"s3://model-bucket/churn_model\")\n",
    "\n",
    "# Пример использования\n",
    "new_data = spark.read.parquet(\"s3://data-lake/new_customers/\")\n",
    "predictions = loaded_model.transform(new_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Оптимизация производительности**\n",
    "### **4.1. Настройка партиционирования**\n",
    "```python\n",
    "features.repartition(100, \"region_id\").write.parquet(\n",
    "    \"s3://feature-store/churn-features\",\n",
    "    partitionBy=\"region_id\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "```\n",
    "\n",
    "### **4.2. Кэширование часто используемых данных**\n",
    "```python\n",
    "spark.sql(\"CACHE TABLE customers_features\")\n",
    "```\n",
    "\n",
    "### **4.3. Мониторинг через Spark UI**\n",
    "```python\n",
    "# Доступно по адресу:\n",
    "# http://<driver-node>:4040\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Задание для самостоятельной доработки**\n",
    "1. **Добавьте обработку потоковых данных** (новые взаимодействия клиентов)\n",
    "2. **Реализуйте A/B тестирование** моделей\n",
    "3. **Настройте алертинг** при падении качества модели\n",
    "4. **Оптимизируйте стоимость** выполнения пайплайна\n",
    "\n",
    "**Пример решения для потоковой обработки**:\n",
    "```python\n",
    "streaming_data = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", \"customer-interactions\") \\\n",
    "    .load()\n",
    "\n",
    "def process_stream(batch_df, batch_id):\n",
    "    # Ваша логика обработки\n",
    "    pass\n",
    "\n",
    "query = streaming_data.writeStream \\\n",
    "    .foreachBatch(process_stream) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Чеклист завершения проекта**\n",
    "- [ ] ETL-пайплайн для сырых данных\n",
    "- [ ] Feature Store с актуальными признаками\n",
    "- [ ] Обученная и протестированная модель\n",
    "- [ ] Визуализация ключевых метрик\n",
    "- [ ] Документация к решению\n",
    "- [ ] Настройки мониторинга\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Дальнейшее развитие проекта**\n",
    "1. **Интеграция с MLflow** для трекинга экспериментов\n",
    "2. **Реализация CI/CD** для пайплайна\n",
    "3. **Переход на Delta Lake** для надежного хранения\n",
    "4. **Автоматическое переобучение** модели\n",
    "\n",
    "**Поздравляю!** Вы завершили интенсивный курс по PySpark и теперь готовы к реальным задачам в области больших данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
