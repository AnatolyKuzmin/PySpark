{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cc9b16-83bc-49be-a7d5-5f6d6cb635a1",
   "metadata": {},
   "source": [
    "# 2. Продвинутые операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8e4d63-7a2d-4b2b-b6b0-a96039bdb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'  # Обход проверки пользователя\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9ef16-8985-4bb8-945c-aa836c7fe09f",
   "metadata": {},
   "source": [
    "## 2.1 Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e67184-586f-410b-87b3-e5e89fb3ff7c",
   "metadata": {},
   "source": [
    "Оконные функции (Window Functions) в PySpark позволяют выполнять вычисления над группами строк, сохраняя при этом индивидуальность каждой строки.\n",
    "\n",
    "Основные компоненты оконных функций:\n",
    "1. Оконная спецификация (WindowSpec) - определяет, какие строки будут включены в рамки окна для каждой строки\n",
    "2. Оконная функция - функция, которая применяется к данным в рамках окна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14de29-a4b1-40a3-b0fa-82f93f1782e0",
   "metadata": {},
   "source": [
    "**Ранжирование** (rank(), dense_rank(), row_number(), percent_rank())  \n",
    "**Агрегатные функции** (sum(), avg(), max(), min())  \n",
    "**Смещение** (lag(), lead())  \n",
    "**Аналитические функции** (first(), last(), cume_dist(), ntile())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e9389a-467a-4123-b971-fdeabfe507ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----------+----+\n",
      "|user_id|      date|amount|prev_amount|diff|\n",
      "+-------+----------+------+-----------+----+\n",
      "|      1|2023-01-10|   100|       NULL|NULL|\n",
      "|      1|2023-01-15|   200|        100| 100|\n",
      "|      1|2023-01-20|   300|        200| 100|\n",
      "|      2|2023-01-12|    50|       NULL|NULL|\n",
      "+-------+----------+------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Задача: Найти разницу между текущей и предыдущей покупкой для каждого пользователя.\n",
    "\n",
    "# Создаём DataFrame с покупками\n",
    "sales = spark.createDataFrame([\n",
    "    (1, \"2023-01-10\", 100),\n",
    "    (1, \"2023-01-15\", 200),\n",
    "    (2, \"2023-01-12\", 50),\n",
    "    (1, \"2023-01-20\", 300)\n",
    "], [\"user_id\", \"date\", \"amount\"])\n",
    "\n",
    "# Определяем окно\n",
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(\"user_id\").orderBy(\"date\")\n",
    "\n",
    "# Добавляем разницу с предыдущей покупкой\n",
    "from pyspark.sql.functions import lag, col\n",
    "sales_with_diff = sales.withColumn(\"prev_amount\", lag(\"amount\").over(window)).withColumn(\"diff\", col(\"amount\") - col(\"prev_amount\"))\n",
    "sales_with_diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2c3323-79b2-4711-8ffc-d8ca0229d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+-----+----+\n",
      "|product_id|  name|   category|price|rank|\n",
      "+----------+------+-----------+-----+----+\n",
      "|         1|Laptop|Electronics|  999|   1|\n",
      "|         2| Phone|Electronics|  699|   2|\n",
      "|         3|  Desk|  Furniture|  200|   1|\n",
      "|         4| Chair|  Furniture|  150|   2|\n",
      "+----------+------+-----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Топ-3 товара по категориям\n",
    "\n",
    "# Данные о товарах\n",
    "products = spark.createDataFrame([\n",
    "    (1, \"Laptop\", \"Electronics\", 999),\n",
    "    (2, \"Phone\", \"Electronics\", 699),\n",
    "    (3, \"Desk\", \"Furniture\", 200),\n",
    "    (4, \"Chair\", \"Furniture\", 150)\n",
    "], [\"product_id\", \"name\", \"category\", \"price\"])\n",
    "\n",
    "# Окно для ранжирования\n",
    "window = Window.partitionBy(\"category\").orderBy(col(\"price\").desc())\n",
    "\n",
    "# Топ-3 в каждой категории\n",
    "from pyspark.sql.functions import dense_rank\n",
    "top_products = products.withColumn(\"rank\", dense_rank().over(window)).filter(col(\"rank\") <= 3)\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ca967-07c1-4419-922b-a48575154614",
   "metadata": {},
   "source": [
    "## 2.2 Работа с датами и строками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892b443-3ceb-4b5c-868d-544c82735c2a",
   "metadata": {},
   "source": [
    "Обработка дат и времени:\n",
    "- Парсинг строк в даты: to_date(), to_timestamp()\n",
    "- Извлечение компонентов: year(), month(), dayofweek()\n",
    "- Арифметика: datediff(), date_add(), months_between()\n",
    "- Форматирование: date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfc74ee-f800-464d-8152-717fba7adf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+----+-----+-----------+\n",
      "|order_id|order_date|amount|parsed_date|year|month|day_of_week|\n",
      "+--------+----------+------+-----------+----+-----+-----------+\n",
      "|       1|2023-01-15| 150.0| 2023-01-15|2023|    1|          1|\n",
      "|       2|15-02-2023| 200.0| 2023-02-15|2023|    2|          4|\n",
      "|       3|2023/03/20| 99.99| 2023-03-20|2023|    3|          2|\n",
      "+--------+----------+------+-----------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Анализ продаж по времени\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data\").getOrCreate()\n",
    "\n",
    "# Создаём DataFrame с датами\n",
    "sales_data = [\n",
    "    (1, \"2023-01-15\", 150.0),\n",
    "    (2, \"15-02-2023\", 200.0),  # Нестандартный формат\n",
    "    (3, \"2023/03/20\", 99.99)\n",
    "]\n",
    "df = spark.createDataFrame(sales_data, [\"order_id\", \"order_date\", \"amount\"])\n",
    "\n",
    "# Приводим даты к единому формату (yyyy-MM-dd)\n",
    "df = df.withColumn(\"parsed_date\", \n",
    "      to_date(col(\"order_date\"), \"yyyy-MM-dd\")) \\\n",
    "      .withColumn(\"parsed_date\", \n",
    "      coalesce(col(\"parsed_date\"), \n",
    "               to_date(col(\"order_date\"), \"dd-MM-yyyy\"),\n",
    "               to_date(col(\"order_date\"), \"yyyy/MM/dd\")))\n",
    "\n",
    "# Извлекаем год, месяц и день недели\n",
    "df = df.withColumn(\"year\", year(\"parsed_date\")) \\\n",
    "       .withColumn(\"month\", month(\"parsed_date\")) \\\n",
    "       .withColumn(\"day_of_week\", dayofweek(\"parsed_date\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b8d88a-c022-4215-9fcf-f60894e4adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+----+-----+-----------+---------------------+\n",
      "|order_id|order_date|amount|parsed_date|year|month|day_of_week|days_since_last_order|\n",
      "+--------+----------+------+-----------+----+-----+-----------+---------------------+\n",
      "|       1|2023-01-15| 150.0| 2023-01-15|2023|    1|          1|                 NULL|\n",
      "|       2|15-02-2023| 200.0| 2023-02-15|2023|    2|          4|                   31|\n",
      "|       3|2023/03/20| 99.99| 2023-03-20|2023|    3|          2|                   33|\n",
      "+--------+----------+------+-----------+----+-----+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Расчёт дней между заказами\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"year\").orderBy(\"parsed_date\")\n",
    "df = df.withColumn(\"days_since_last_order\", \n",
    "      datediff(col(\"parsed_date\"), lag(\"parsed_date\").over(window)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd16501-1450-4a58-90d4-b1b2a8d7eff3",
   "metadata": {},
   "source": [
    "Работа со строками:\n",
    "- Базовые операции: concat(), substring(), trim()\n",
    "- Регулярные выражения: regexp_extract(), regexp_replace()\n",
    "- Проверки: startswith(), endswith(), contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebe61f4-eefa-4ae6-b58f-2f06446d976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+\n",
      "|user_id|            email|     domain|\n",
      "+-------+-----------------+-----------+\n",
      "|      1|alice@example.com|example.com|\n",
      "|      2|    bob@gmail.com|  gmail.com|\n",
      "+-------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Очистка и анализ текста\n",
    "\n",
    "# Извлечение домена из email\n",
    "users = spark.createDataFrame([\n",
    "    (1, \"alice@example.com\"),\n",
    "    (2, \"bob@gmail.com\")\n",
    "], [\"user_id\", \"email\"])\n",
    "\n",
    "users = users.withColumn(\"domain\", \n",
    "      regexp_extract(col(\"email\"), \"@(.+)$\", 1))\n",
    "\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fd9b00-8d48-43dc-a3ba-f8f405b0f4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|comment_id|              text|        clean_text|\n",
      "+----------+------------------+------------------+\n",
      "|         1|      This is bad!|      This is ***!|\n",
      "|         2|Worst product ever|Worst product ever|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Замена цензурой нецензурных слов\n",
    "comments = spark.createDataFrame([\n",
    "    (1, \"This is bad!\"),\n",
    "    (2, \"Worst product ever\")\n",
    "], [\"comment_id\", \"text\"])\n",
    "\n",
    "bad_words = [\"bad\", \"worst\"]\n",
    "pattern = \"|\".join(bad_words)\n",
    "comments = comments.withColumn(\"clean_text\", \n",
    "      regexp_replace(col(\"text\"), pattern, \"***\"))\n",
    "\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11a19102-e941-4222-877f-975a0784a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+-----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|  surname|name_length|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+-----------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|   Braund|         23|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|  Cumings|         51|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|Heikkinen|         22|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S| Futrelle|         44|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|    Allen|         24|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка CSV с заголовком и выводом схемы\n",
    "df_Titanic = spark.read.csv(\"data/train.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Разделяем \"Surname, Title Name\" и извлекаем фамилию\n",
    "df_Titanic = df_Titanic.withColumn(\"surname\", \n",
    "      split(col(\"Name\"), \",\")[0])\n",
    "\n",
    "# Длина имени\n",
    "df_Titanic = df_Titanic.withColumn(\"name_length\", \n",
    "      length(col(\"Name\")))\n",
    "\n",
    "df_Titanic.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e6b26-7e20-4291-99d3-d47783340d75",
   "metadata": {},
   "source": [
    "## 2.3 Оптимизация (партиционирование, кэширование)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfdb95-ba5e-4c70-8b33-da0a89e151f9",
   "metadata": {},
   "source": [
    "Партиционирование — разделение данных на части (партиции) для параллельной обработки. Большие файлы (например, 100+ ГБ CSV/Parquet). Частые фильтры по определённым колонкам (например, по дате)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576027d0-5401-43cc-b248-536d8b398fcb",
   "metadata": {},
   "source": [
    "Пример\n",
    "```\n",
    "# Запись данных с партиционированием по колонке 'Pclass' (Titanic dataset)\n",
    "df.write.partitionBy(\"Pclass\").parquet(\"data/titanic_partitioned\")\n",
    "\n",
    "# Чтение только партиции Pclass=1 (ускоряет загрузку в 3+ раза)\n",
    "df_pclass1 = spark.read.parquet(\"data/titanic_partitioned/Pclass=1\")\n",
    "```\n",
    "\n",
    "```\n",
    "# Репартиционирование в 200 партиций (для равномерного распределения)\n",
    "df_repartitioned = df.repartition(200)\n",
    "\n",
    "# Партиционирование по колонке + указание числа партиций\n",
    "df_repartitioned = df.repartition(10, \"Pclass\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a5f9d-7098-45d8-a46e-a3daf689cc3f",
   "metadata": {},
   "source": [
    "Кэширование сохраняет данные в памяти или на диске для повторного использования. Если DataFrame используется более 2-х раз. Перед многократными итерациями (например, в ML-циклах).  \n",
    "Уровни хранения:\n",
    "- `MEMORY_ONLY` — только в памяти (быстро, но риск OOM).\n",
    "- `MEMORY_AND_DISK` — сначала память, потом диск (надежнее).\n",
    "- `DISK_ONLY` — только диск (медленно, но безопасно)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af0fe5-ae7b-43bb-9d5e-45c6433af2c4",
   "metadata": {},
   "source": [
    "Пример\n",
    "```\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Кэширование в памяти (аналог .cache())\n",
    "df.cache()  \n",
    "\n",
    "# Явное указание уровня\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Проверка статуса\n",
    "df.is_cached  # -> True\n",
    "\n",
    "# Освобождение памяти\n",
    "df.unpersist()\n",
    "```\n",
    "\n",
    "```\n",
    "# Плохо: DataFrame читается 2 раза\n",
    "df.filter(df.Age > 30).count()\n",
    "df.filter(df.Age > 30).show()\n",
    "\n",
    "# Хорошо: Кэшируем перед повторным использованием\n",
    "filtered_df = df.filter(df.Age > 30).persist()\n",
    "filtered_df.count()\n",
    "filtered_df.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9387807-fbc5-4747-a1f5-466a91bd341b",
   "metadata": {},
   "source": [
    "### Настройка памяти и параметров Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea2f8a-a258-4c5a-9552-7c4ba0f519a6",
   "metadata": {},
   "source": [
    "```\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimization\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\  # Память на executor\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\    # Память на driver\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\  # Число партиций после shuffle\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fd8eb-270d-409e-afd2-5d34ddbefa78",
   "metadata": {},
   "source": [
    "Shuffle — перераспределение данных между узлами (например, при groupBy или join).  \n",
    "Как уменьшить нагрузку:\n",
    "- Увеличьте spark.sql.shuffle.partitions (по умолчанию 200).\n",
    "- Используйте broadcast для маленьких таблиц:\n",
    "```\n",
    "from pyspark.sql.functions import broadcast\n",
    "df1.join(broadcast(df2), \"key\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f2fa5-7d15-4ca9-97e9-269cba56239d",
   "metadata": {},
   "source": [
    "Мониторинг и отладка. Откройте в браузере: `http://localhost:4040` (для локального режима)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad0645-1e6c-4b14-ba99-1a507dec0a84",
   "metadata": {},
   "source": [
    "| Ошибка | Причина | Решение |\n",
    "| -- | -- | -- |\n",
    "| OutOfMemoryError | Нехватка памяти | Увеличьте spark.executor.memory |\n",
    "| Data skew | Неравномерное распределение данных | Используйте repartition или salting |\n",
    "| Slow JOIN | Большие таблицы без оптимизации | Примените broadcast для малых таблиц |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f5688-a85f-4043-aaee-be5bfadf960f",
   "metadata": {},
   "source": [
    "## 2.4 UDF (пользовательские функции)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f58e9a-e2e3-491a-b0f1-a06dd3dc5cfb",
   "metadata": {},
   "source": [
    "User-Defined Functions — это пользовательская функция, которую можно применять к колонкам DataFrame.\n",
    "\n",
    "Когда встроенных функций Spark недостаточно. Для применения сложной Python-логики (например, NLP, машинное обучение).\n",
    "\n",
    "UDF работают медленнее встроенных функций (из-за накладных расходов на сериализацию). Для ускорения используют Pandas UDF (векторизованные функции)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad30260-3823-413b-8490-e6dfa6fae934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------+\n",
      "|   name|age|age_category|\n",
      "+-------+---+------------+\n",
      "|  Alice| 25|       adult|\n",
      "|    Bob| 60|      senior|\n",
      "|Charlie| 15|       child|\n",
      "+-------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Функция для категоризации возраста.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UDFs\").getOrCreate()\n",
    "\n",
    "# Данные\n",
    "data = [(\"Alice\", 25), (\"Bob\", 60), (\"Charlie\", 15)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# Шаг 1: Создаём Python-функцию\n",
    "def age_category(age):\n",
    "    if age < 18:\n",
    "        return \"child\"\n",
    "    elif age >= 60:\n",
    "        return \"senior\"\n",
    "    else:\n",
    "        return \"adult\"\n",
    "\n",
    "# Шаг 2: Регистрируем UDF (указываем тип возвращаемого значения)\n",
    "age_category_udf = udf(age_category, StringType())\n",
    "\n",
    "# Шаг 3: Применяем к DataFrame\n",
    "df = df.withColumn(\"age_category\", age_category_udf(df[\"age\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4feab2-ccf3-4fd1-8428-49972a16d7a2",
   "metadata": {},
   "source": [
    "Векторизованные Pandas UDF (быстрые). Pandas UDF работают в 10-100 раз быстрее обычных UDF, т.к. обрабатывают данные партиями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23c4b70a-908d-4675-99b3-bd04a84ec724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------+-----------+\n",
      "|   name|age|age_category|age_squared|\n",
      "+-------+---+------------+-----------+\n",
      "|  Alice| 25|       adult|        625|\n",
      "|    Bob| 60|      senior|       3600|\n",
      "|Charlie| 15|       child|        225|\n",
      "+-------+---+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Расчёт квадрата числа через Pandas UDF.\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "# Шаг 1: Создаём векторную функцию\n",
    "@pandas_udf(IntegerType())\n",
    "def square_udf(age: pd.Series) -> pd.Series:\n",
    "    return age ** 2\n",
    "\n",
    "# Шаг 2: Применяем\n",
    "df = df.withColumn(\"age_squared\", square_udf(df[\"age\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16158c-f805-4497-9877-a9dcef955e5a",
   "metadata": {},
   "source": [
    "Кэширование результатов. Если UDF вызывается многократно, закэшируйте DataFrame: `df.cache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cfa9db-6cde-4eb2-8655-365782abfe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas UDF: 8.301344633102417 sec\n",
      "Regular UDF: 8.37052869796753 sec\n"
     ]
    }
   ],
   "source": [
    "# UDF, которая преобразует колонку Name в формат ФАМИЛИЯ, \n",
    "# Имя (например, \"Braund, Mr. Owen Harris\" → \"BRAUND, Owen\").\n",
    "def format_name(name):\n",
    "    surname = name.split(\",\")[0].upper()\n",
    "    first_name = name.split(\",\")[1].split(\".\")[1].strip()\n",
    "    return f\"{surname}, {first_name}\"\n",
    "\n",
    "format_name_udf = udf(format_name, StringType())\n",
    "df_Titanic = df_Titanic.withColumn(\"formatted_name\", format_name_udf(df_Titanic[\"Name\"]))\n",
    "\n",
    "# Pandas UDF для расчёта длины строки в колонке Name.\n",
    "@pandas_udf(IntegerType())\n",
    "def name_length_udf(names: pd.Series) -> pd.Series:\n",
    "    return names.str.len()\n",
    "\n",
    "df_Titanic = df_Titanic.withColumn(\"name_length\", name_length_udf(df_Titanic[\"Name\"]))\n",
    "\n",
    "# 3. Тест скорости\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "df.withColumn(\"name_length\", name_length_udf(df[\"Name\"])).count()\n",
    "print(f\"Pandas UDF: {time.time() - start} sec\")\n",
    "\n",
    "start = time.time()\n",
    "df.withColumn(\"formatted_name\", format_name_udf(df[\"Name\"])).count()\n",
    "print(f\"Regular UDF: {time.time() - start} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e21f63-5441-4d7e-a652-70789cc9aab0",
   "metadata": {},
   "source": [
    "## 2.5 Чтение/запись в разных форматах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf1de3-d042-49ab-81d4-1c45649718b0",
   "metadata": {},
   "source": [
    "| Формат | Описание | Плюсы | Минусы |\n",
    "| - | - | - | - |\n",
    "| CSV | Текстовый формат с разделителями | Простота, человекочитаемость | Нет схемы, медленный |\n",
    "| JSON | Полуструктурированный формат | Поддержка вложенных данных | Медленный, большой объём |\n",
    "| Parquet | Бинарный колоночный формат | Быстрый, сжатый, с поддержкой схемы | Нечитаем для человека |\n",
    "| Avro | Бинарный формат с схемой | Эффективен для сериализации | Требует схемы |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec016f70-8d93-4040-bfcb-41f13b8f3048",
   "metadata": {},
   "source": [
    "### Чтение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462e59f-b918-4e15-82ce-f8b72ffb9974",
   "metadata": {},
   "source": [
    "Чтение данных CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c26be8-d48b-43c9-a87c-4228f499bcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.2833|  C85|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Использовать первую строку как заголовок\n",
    "# Автоматически определить типы данных\n",
    "# Указать разделитель\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv(\"data/train.csv\")\n",
    "\n",
    "df_csv.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103e73d-5d46-412c-8443-0c3a53ee90f5",
   "metadata": {},
   "source": [
    "Для больших файлов лучше явно указать схему (ускоряет загрузку)\n",
    "```\n",
    "schema = StructType([\n",
    "    StructField(\"PassengerId\", IntegerType()),\n",
    "    StructField(\"Survived\", IntegerType()),\n",
    "    # ... остальные поля\n",
    "])\n",
    "\n",
    "df_csv = spark.read.schema(schema).csv(\"data/train.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eba4fc-3128-4d61-9675-04bda89cdf9f",
   "metadata": {},
   "source": [
    "Чтение данных JSON\n",
    "```\n",
    "df_json = spark.read \\\n",
    "    .option(\"multiLine\", \"true\") \\      # Для многострочных JSON\n",
    "    .json(\"data/amazon_sales.json\")\n",
    "\n",
    "df_json.printSchema()\n",
    "```\n",
    "Если JSON содержит вложенные структуры, используйте select + explode:\n",
    "```\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_exploded = df_json.select(\"user_id\", explode(\"orders\").alias(\"order\"))\n",
    "df_exploded.select(\"order.*\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6a333-214b-4583-94c2-b8c1b710c8ba",
   "metadata": {},
   "source": [
    "Parquet (рекомендуемый формат для Big Data)\n",
    "```\n",
    "df_parquet = spark.read.parquet(\"data/titanic.parquet\")\n",
    "```\n",
    "Почему Parquet?  \n",
    "- Сжатие данных (экономия места).\n",
    "- Колоночная организация (быстрые агрегации).\n",
    "- Поддержка схемы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565143-00af-4259-90da-bbade1d3d71b",
   "metadata": {},
   "source": [
    "Чтение данных Avro (требует пакета spark-avro)\n",
    "```\n",
    "df_avro = spark.read \\\n",
    "    .format(\"avro\") \\\n",
    "    .load(\"data/titanic.avro\")\n",
    "```\n",
    "\n",
    "Установка:\n",
    "```\n",
    "spark-submit --packages org.apache.spark:spark-avro_2.12:3.5.0 your_script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfc149-f48d-4d47-a237-fee04ba4f8cb",
   "metadata": {},
   "source": [
    "### Запись данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d7c86-bc07-4852-9ff6-29d291948e0f",
   "metadata": {},
   "source": [
    "CSV \n",
    "```\n",
    "df_csv.write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"|\") \\    # Изменяем разделитель\n",
    "    .mode(\"overwrite\") \\           # Перезаписать, если файл существует\n",
    "    .csv(\"output/titanic_csv\")\n",
    "```\n",
    "Parquet \n",
    "```\n",
    "df.write \\\n",
    "    .partitionBy(\"Pclass\") \\       # Партиционирование по колонке\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"output/titanic_parquet\")\n",
    "```\n",
    "JSON\n",
    "```\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"output/titanic_json\")\n",
    "```\n",
    "Avro\n",
    "```\n",
    "df.write \\\n",
    "    .format(\"avro\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"output/titanic.avro\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3badb05d-a0d7-4576-a248-0cb95b667a13",
   "metadata": {},
   "source": [
    "### Конвертация между форматами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f88ee8-403c-4748-959b-d2fd3ae29f9e",
   "metadata": {},
   "source": [
    "Конвертация CSV в Parquet (для ускорения последующих запросов).\n",
    "```\n",
    "df_csv = spark.read.csv(\"data/titanic.csv\", header=True, inferSchema=True)\n",
    "df_csv.write.parquet(\"data/titanic_parquet\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
