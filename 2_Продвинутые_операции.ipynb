{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cc9b16-83bc-49be-a7d5-5f6d6cb635a1",
   "metadata": {},
   "source": [
    "# 2. Продвинутые операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8e4d63-7a2d-4b2b-b6b0-a96039bdb794",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2840)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2837)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2927)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      8\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.host\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocalhost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.executor.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:203\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    201\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:296\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:421\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1581\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1583\u001b[39m     args_command +\\\n\u001b[32m   1584\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1586\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\r\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2840)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2837)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2927)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'  # Обход проверки пользователя\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9ef16-8985-4bb8-945c-aa836c7fe09f",
   "metadata": {},
   "source": [
    "## 2.1 Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e67184-586f-410b-87b3-e5e89fb3ff7c",
   "metadata": {},
   "source": [
    "Оконные функции (Window Functions) в PySpark позволяют выполнять вычисления над группами строк, сохраняя при этом индивидуальность каждой строки.\n",
    "\n",
    "Основные компоненты оконных функций:\n",
    "1. Оконная спецификация (WindowSpec) - определяет, какие строки будут включены в рамки окна для каждой строки\n",
    "2. Оконная функция - функция, которая применяется к данным в рамках окна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14de29-a4b1-40a3-b0fa-82f93f1782e0",
   "metadata": {},
   "source": [
    "**Ранжирование** (rank(), dense_rank(), row_number(), percent_rank())  \n",
    "**Агрегатные функции** (sum(), avg(), max(), min())  \n",
    "**Смещение** (lag(), lead())  \n",
    "**Аналитические функции** (first(), last(), cume_dist(), ntile())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9389a-467a-4123-b971-fdeabfe507ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задача: Найти разницу между текущей и предыдущей покупкой для каждого пользователя.\n",
    "\n",
    "# Создаём DataFrame с покупками\n",
    "sales = spark.createDataFrame([\n",
    "    (1, \"2023-01-10\", 100),\n",
    "    (1, \"2023-01-15\", 200),\n",
    "    (2, \"2023-01-12\", 50),\n",
    "    (1, \"2023-01-20\", 300)\n",
    "], [\"user_id\", \"date\", \"amount\"])\n",
    "\n",
    "# Определяем окно\n",
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(\"user_id\").orderBy(\"date\")\n",
    "\n",
    "# Добавляем разницу с предыдущей покупкой\n",
    "from pyspark.sql.functions import lag, col\n",
    "sales_with_diff = sales.withColumn(\"prev_amount\", lag(\"amount\").over(window)).withColumn(\"diff\", col(\"amount\") - col(\"prev_amount\"))\n",
    "sales_with_diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c3323-79b2-4711-8ffc-d8ca0229d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Топ-3 товара по категориям\n",
    "\n",
    "# Данные о товарах\n",
    "products = spark.createDataFrame([\n",
    "    (1, \"Laptop\", \"Electronics\", 999),\n",
    "    (2, \"Phone\", \"Electronics\", 699),\n",
    "    (3, \"Desk\", \"Furniture\", 200),\n",
    "    (4, \"Chair\", \"Furniture\", 150)\n",
    "], [\"product_id\", \"name\", \"category\", \"price\"])\n",
    "\n",
    "# Окно для ранжирования\n",
    "window = Window.partitionBy(\"category\").orderBy(col(\"price\").desc())\n",
    "\n",
    "# Топ-3 в каждой категории\n",
    "from pyspark.sql.functions import dense_rank\n",
    "top_products = products.withColumn(\"rank\", dense_rank().over(window)).filter(col(\"rank\") <= 3)\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ca967-07c1-4419-922b-a48575154614",
   "metadata": {},
   "source": [
    "## 2.2 Работа с датами и строками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892b443-3ceb-4b5c-868d-544c82735c2a",
   "metadata": {},
   "source": [
    "Обработка дат и времени:\n",
    "- Парсинг строк в даты: to_date(), to_timestamp()\n",
    "- Извлечение компонентов: year(), month(), dayofweek()\n",
    "- Арифметика: datediff(), date_add(), months_between()\n",
    "- Форматирование: date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc74ee-f800-464d-8152-717fba7adf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ продаж по времени\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data\").getOrCreate()\n",
    "\n",
    "# Создаём DataFrame с датами\n",
    "sales_data = [\n",
    "    (1, \"2023-01-15\", 150.0),\n",
    "    (2, \"15-02-2023\", 200.0),  # Нестандартный формат\n",
    "    (3, \"2023/03/20\", 99.99)\n",
    "]\n",
    "df = spark.createDataFrame(sales_data, [\"order_id\", \"order_date\", \"amount\"])\n",
    "\n",
    "# Приводим даты к единому формату (yyyy-MM-dd)\n",
    "df = df.withColumn(\"parsed_date\", \n",
    "      to_date(col(\"order_date\"), \"yyyy-MM-dd\")) \\\n",
    "      .withColumn(\"parsed_date\", \n",
    "      coalesce(col(\"parsed_date\"), \n",
    "               to_date(col(\"order_date\"), \"dd-MM-yyyy\"),\n",
    "               to_date(col(\"order_date\"), \"yyyy/MM/dd\")))\n",
    "\n",
    "# Извлекаем год, месяц и день недели\n",
    "df = df.withColumn(\"year\", year(\"parsed_date\")) \\\n",
    "       .withColumn(\"month\", month(\"parsed_date\")) \\\n",
    "       .withColumn(\"day_of_week\", dayofweek(\"parsed_date\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8d88a-c022-4215-9fcf-f60894e4adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчёт дней между заказами\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"year\").orderBy(\"parsed_date\")\n",
    "df = df.withColumn(\"days_since_last_order\", \n",
    "      datediff(col(\"parsed_date\"), lag(\"parsed_date\").over(window)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd16501-1450-4a58-90d4-b1b2a8d7eff3",
   "metadata": {},
   "source": [
    "Работа со строками:\n",
    "- Базовые операции: concat(), substring(), trim()\n",
    "- Регулярные выражения: regexp_extract(), regexp_replace()\n",
    "- Проверки: startswith(), endswith(), contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe61f4-eefa-4ae6-b58f-2f06446d976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка и анализ текста\n",
    "\n",
    "# Извлечение домена из email\n",
    "users = spark.createDataFrame([\n",
    "    (1, \"alice@example.com\"),\n",
    "    (2, \"bob@gmail.com\")\n",
    "], [\"user_id\", \"email\"])\n",
    "\n",
    "users = users.withColumn(\"domain\", \n",
    "      regexp_extract(col(\"email\"), \"@(.+)$\", 1))\n",
    "\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd9b00-8d48-43dc-a3ba-f8f405b0f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замена цензурой нецензурных слов\n",
    "comments = spark.createDataFrame([\n",
    "    (1, \"This is bad!\"),\n",
    "    (2, \"Worst product ever\")\n",
    "], [\"comment_id\", \"text\"])\n",
    "\n",
    "bad_words = [\"bad\", \"worst\"]\n",
    "pattern = \"|\".join(bad_words)\n",
    "comments = comments.withColumn(\"clean_text\", \n",
    "      regexp_replace(col(\"text\"), pattern, \"***\"))\n",
    "\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a19102-e941-4222-877f-975a0784a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка CSV с заголовком и выводом схемы\n",
    "df_Titanic = spark.read.csv(\"data/train.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Разделяем \"Surname, Title Name\" и извлекаем фамилию\n",
    "df_Titanic = df_Titanic.withColumn(\"surname\", \n",
    "      split(col(\"Name\"), \",\")[0])\n",
    "\n",
    "# Длина имени\n",
    "df_Titanic = df_Titanic.withColumn(\"name_length\", \n",
    "      length(col(\"Name\")))\n",
    "\n",
    "df_Titanic.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e6b26-7e20-4291-99d3-d47783340d75",
   "metadata": {},
   "source": [
    "## 2.3 Оптимизация (партиционирование, кэширование)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfdb95-ba5e-4c70-8b33-da0a89e151f9",
   "metadata": {},
   "source": [
    "Партиционирование — разделение данных на части (партиции) для параллельной обработки. Большие файлы (например, 100+ ГБ CSV/Parquet). Частые фильтры по определённым колонкам (например, по дате)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576027d0-5401-43cc-b248-536d8b398fcb",
   "metadata": {},
   "source": [
    "Пример\n",
    "```\n",
    "# Запись данных с партиционированием по колонке 'Pclass' (Titanic dataset)\n",
    "df.write.partitionBy(\"Pclass\").parquet(\"data/titanic_partitioned\")\n",
    "\n",
    "# Чтение только партиции Pclass=1 (ускоряет загрузку в 3+ раза)\n",
    "df_pclass1 = spark.read.parquet(\"data/titanic_partitioned/Pclass=1\")\n",
    "```\n",
    "\n",
    "```\n",
    "# Репартиционирование в 200 партиций (для равномерного распределения)\n",
    "df_repartitioned = df.repartition(200)\n",
    "\n",
    "# Партиционирование по колонке + указание числа партиций\n",
    "df_repartitioned = df.repartition(10, \"Pclass\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a5f9d-7098-45d8-a46e-a3daf689cc3f",
   "metadata": {},
   "source": [
    "Кэширование сохраняет данные в памяти или на диске для повторного использования. Если DataFrame используется более 2-х раз. Перед многократными итерациями (например, в ML-циклах).  \n",
    "Уровни хранения:\n",
    "- `MEMORY_ONLY` — только в памяти (быстро, но риск OOM).\n",
    "- `MEMORY_AND_DISK` — сначала память, потом диск (надежнее).\n",
    "- `DISK_ONLY` — только диск (медленно, но безопасно)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af0fe5-ae7b-43bb-9d5e-45c6433af2c4",
   "metadata": {},
   "source": [
    "Пример\n",
    "```\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Кэширование в памяти (аналог .cache())\n",
    "df.cache()  \n",
    "\n",
    "# Явное указание уровня\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Проверка статуса\n",
    "df.is_cached  # -> True\n",
    "\n",
    "# Освобождение памяти\n",
    "df.unpersist()\n",
    "```\n",
    "\n",
    "```\n",
    "# Плохо: DataFrame читается 2 раза\n",
    "df.filter(df.Age > 30).count()\n",
    "df.filter(df.Age > 30).show()\n",
    "\n",
    "# Хорошо: Кэшируем перед повторным использованием\n",
    "filtered_df = df.filter(df.Age > 30).persist()\n",
    "filtered_df.count()\n",
    "filtered_df.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9387807-fbc5-4747-a1f5-466a91bd341b",
   "metadata": {},
   "source": [
    "### Настройка памяти и параметров Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea2f8a-a258-4c5a-9552-7c4ba0f519a6",
   "metadata": {},
   "source": [
    "```\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimization\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\  # Память на executor\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\    # Память на driver\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\  # Число партиций после shuffle\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fd8eb-270d-409e-afd2-5d34ddbefa78",
   "metadata": {},
   "source": [
    "Shuffle — перераспределение данных между узлами (например, при groupBy или join).  \n",
    "Как уменьшить нагрузку:\n",
    "- Увеличьте spark.sql.shuffle.partitions (по умолчанию 200).\n",
    "- Используйте broadcast для маленьких таблиц:\n",
    "```\n",
    "from pyspark.sql.functions import broadcast\n",
    "df1.join(broadcast(df2), \"key\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f2fa5-7d15-4ca9-97e9-269cba56239d",
   "metadata": {},
   "source": [
    "Мониторинг и отладка. Откройте в браузере: `http://localhost:4040` (для локального режима)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad0645-1e6c-4b14-ba99-1a507dec0a84",
   "metadata": {},
   "source": [
    "| Ошибка | Причина | Решение |\n",
    "| -- | -- | -- |\n",
    "| OutOfMemoryError | Нехватка памяти | Увеличьте spark.executor.memory |\n",
    "| Data skew | Неравномерное распределение данных | Используйте repartition или salting |\n",
    "| Slow JOIN | Большие таблицы без оптимизации | Примените broadcast для малых таблиц |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f5688-a85f-4043-aaee-be5bfadf960f",
   "metadata": {},
   "source": [
    "## 2.4 UDF (пользовательские функции)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f58e9a-e2e3-491a-b0f1-a06dd3dc5cfb",
   "metadata": {},
   "source": [
    "User-Defined Functions — это пользовательская функция, которую можно применять к колонкам DataFrame.\n",
    "\n",
    "Когда встроенных функций Spark недостаточно. Для применения сложной Python-логики (например, NLP, машинное обучение).\n",
    "\n",
    "UDF работают медленнее встроенных функций (из-за накладных расходов на сериализацию). Для ускорения используют Pandas UDF (векторизованные функции)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad30260-3823-413b-8490-e6dfa6fae934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для категоризации возраста.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UDFs\").getOrCreate()\n",
    "\n",
    "# Данные\n",
    "data = [(\"Alice\", 25), (\"Bob\", 60), (\"Charlie\", 15)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# Шаг 1: Создаём Python-функцию\n",
    "def age_category(age):\n",
    "    if age < 18:\n",
    "        return \"child\"\n",
    "    elif age >= 60:\n",
    "        return \"senior\"\n",
    "    else:\n",
    "        return \"adult\"\n",
    "\n",
    "# Шаг 2: Регистрируем UDF (указываем тип возвращаемого значения)\n",
    "age_category_udf = udf(age_category, StringType())\n",
    "\n",
    "# Шаг 3: Применяем к DataFrame\n",
    "df = df.withColumn(\"age_category\", age_category_udf(df[\"age\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4feab2-ccf3-4fd1-8428-49972a16d7a2",
   "metadata": {},
   "source": [
    "Векторизованные Pandas UDF (быстрые). Pandas UDF работают в 10-100 раз быстрее обычных UDF, т.к. обрабатывают данные партиями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4b70a-908d-4675-99b3-bd04a84ec724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчёт квадрата числа через Pandas UDF.\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "# Шаг 1: Создаём векторную функцию\n",
    "@pandas_udf(IntegerType())\n",
    "def square_udf(age: pd.Series) -> pd.Series:\n",
    "    return age ** 2\n",
    "\n",
    "# Шаг 2: Применяем\n",
    "df = df.withColumn(\"age_squared\", square_udf(df[\"age\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16158c-f805-4497-9877-a9dcef955e5a",
   "metadata": {},
   "source": [
    "Кэширование результатов. Если UDF вызывается многократно, закэшируйте DataFrame: `df.cache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfa9db-6cde-4eb2-8655-365782abfe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF, которая преобразует колонку Name в формат ФАМИЛИЯ, \n",
    "# Имя (например, \"Braund, Mr. Owen Harris\" → \"BRAUND, Owen\").\n",
    "def format_name(name):\n",
    "    surname = name.split(\",\")[0].upper()\n",
    "    first_name = name.split(\",\")[1].split(\".\")[1].strip()\n",
    "    return f\"{surname}, {first_name}\"\n",
    "\n",
    "format_name_udf = udf(format_name, StringType())\n",
    "df_Titanic = df_Titanic.withColumn(\"formatted_name\", format_name_udf(df_Titanic[\"Name\"]))\n",
    "\n",
    "# Pandas UDF для расчёта длины строки в колонке Name.\n",
    "@pandas_udf(IntegerType())\n",
    "def name_length_udf(names: pd.Series) -> pd.Series:\n",
    "    return names.str.len()\n",
    "\n",
    "df_Titanic = df_Titanic.withColumn(\"name_length\", name_length_udf(df_Titanic[\"Name\"]))\n",
    "\n",
    "# 3. Тест скорости\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "df.withColumn(\"name_length\", name_length_udf(df[\"Name\"])).count()\n",
    "print(f\"Pandas UDF: {time.time() - start} sec\")\n",
    "\n",
    "start = time.time()\n",
    "df.withColumn(\"formatted_name\", format_name_udf(df[\"Name\"])).count()\n",
    "print(f\"Regular UDF: {time.time() - start} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e21f63-5441-4d7e-a652-70789cc9aab0",
   "metadata": {},
   "source": [
    "## 2.5 Чтение/запись в разных форматах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf1de3-d042-49ab-81d4-1c45649718b0",
   "metadata": {},
   "source": [
    "| Формат | Описание | Плюсы | Минусы |\n",
    "| - | - | - | - |\n",
    "| CSV | Текстовый формат с разделителями | Простота, человекочитаемость | Нет схемы, медленный |\n",
    "| JSON | Полуструктурированный формат | Поддержка вложенных данных | Медленный, большой объём |\n",
    "| Parquet | Бинарный колоночный формат | Быстрый, сжатый, с поддержкой схемы | Нечитаем для человека |\n",
    "| Avro | Бинарный формат с схемой | Эффективен для сериализации | Требует схемы |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c26be8-d48b-43c9-a87c-4228f499bcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be2473-45dd-4a20-9031-b1de8181da76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161581df-c13e-4e11-a098-f644ea99ea29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8770d6f-fe32-4920-96d5-1b7e9d421150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972939c-c5e4-4a6c-a75b-954d07bb5969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99430b66-42cd-4bdf-8ebd-cc887f5d4d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57d1c6-f555-448f-b589-86d8827de098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ea360-3271-4591-b0b6-6e51aead9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26344c85-3639-4369-a143-687d4b2628c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
