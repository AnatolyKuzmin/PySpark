{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cc9b16-83bc-49be-a7d5-5f6d6cb635a1",
   "metadata": {},
   "source": [
    "# 2. Продвинутые операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8e4d63-7a2d-4b2b-b6b0-a96039bdb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'  # Обход проверки пользователя\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9ef16-8985-4bb8-945c-aa836c7fe09f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e67184-586f-410b-87b3-e5e89fb3ff7c",
   "metadata": {},
   "source": [
    "Оконные функции (Window Functions) в PySpark позволяют выполнять вычисления над группами строк, сохраняя при этом индивидуальность каждой строки.\n",
    "\n",
    "Основные компоненты оконных функций:\n",
    "1. Оконная спецификация (WindowSpec) - определяет, какие строки будут включены в рамки окна для каждой строки\n",
    "2. Оконная функция - функция, которая применяется к данным в рамках окна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14de29-a4b1-40a3-b0fa-82f93f1782e0",
   "metadata": {},
   "source": [
    "**Ранжирование** (rank(), dense_rank(), row_number(), percent_rank())  \n",
    "**Агрегатные функции** (sum(), avg(), max(), min())  \n",
    "**Смещение** (lag(), lead())  \n",
    "**Аналитические функции** (first(), last(), cume_dist(), ntile())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e9389a-467a-4123-b971-fdeabfe507ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----------+----+\n",
      "|user_id|      date|amount|prev_amount|diff|\n",
      "+-------+----------+------+-----------+----+\n",
      "|      1|2023-01-10|   100|       NULL|NULL|\n",
      "|      1|2023-01-15|   200|        100| 100|\n",
      "|      1|2023-01-20|   300|        200| 100|\n",
      "|      2|2023-01-12|    50|       NULL|NULL|\n",
      "+-------+----------+------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Задача: Найти разницу между текущей и предыдущей покупкой для каждого пользователя.\n",
    "\n",
    "# Создаём DataFrame с покупками\n",
    "sales = spark.createDataFrame([\n",
    "    (1, \"2023-01-10\", 100),\n",
    "    (1, \"2023-01-15\", 200),\n",
    "    (2, \"2023-01-12\", 50),\n",
    "    (1, \"2023-01-20\", 300)\n",
    "], [\"user_id\", \"date\", \"amount\"])\n",
    "\n",
    "# Определяем окно\n",
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy(\"user_id\").orderBy(\"date\")\n",
    "\n",
    "# Добавляем разницу с предыдущей покупкой\n",
    "from pyspark.sql.functions import lag, col\n",
    "sales_with_diff = sales.withColumn(\"prev_amount\", lag(\"amount\").over(window)).withColumn(\"diff\", col(\"amount\") - col(\"prev_amount\"))\n",
    "sales_with_diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2c3323-79b2-4711-8ffc-d8ca0229d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+-----+----+\n",
      "|product_id|  name|   category|price|rank|\n",
      "+----------+------+-----------+-----+----+\n",
      "|         1|Laptop|Electronics|  999|   1|\n",
      "|         2| Phone|Electronics|  699|   2|\n",
      "|         3|  Desk|  Furniture|  200|   1|\n",
      "|         4| Chair|  Furniture|  150|   2|\n",
      "+----------+------+-----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Топ-3 товара по категориям\n",
    "\n",
    "# Данные о товарах\n",
    "products = spark.createDataFrame([\n",
    "    (1, \"Laptop\", \"Electronics\", 999),\n",
    "    (2, \"Phone\", \"Electronics\", 699),\n",
    "    (3, \"Desk\", \"Furniture\", 200),\n",
    "    (4, \"Chair\", \"Furniture\", 150)\n",
    "], [\"product_id\", \"name\", \"category\", \"price\"])\n",
    "\n",
    "# Окно для ранжирования\n",
    "window = Window.partitionBy(\"category\").orderBy(col(\"price\").desc())\n",
    "\n",
    "# Топ-3 в каждой категории\n",
    "from pyspark.sql.functions import dense_rank\n",
    "top_products = products.withColumn(\"rank\", dense_rank().over(window)).filter(col(\"rank\") <= 3)\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ca967-07c1-4419-922b-a48575154614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Работа с датами и строками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892b443-3ceb-4b5c-868d-544c82735c2a",
   "metadata": {},
   "source": [
    "Обработка дат и времени:\n",
    "- Парсинг строк в даты: to_date(), to_timestamp()\n",
    "- Извлечение компонентов: year(), month(), dayofweek()\n",
    "- Арифметика: datediff(), date_add(), months_between()\n",
    "- Форматирование: date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfc74ee-f800-464d-8152-717fba7adf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+----+-----+-----------+\n",
      "|order_id|order_date|amount|parsed_date|year|month|day_of_week|\n",
      "+--------+----------+------+-----------+----+-----+-----------+\n",
      "|       1|2023-01-15| 150.0| 2023-01-15|2023|    1|          1|\n",
      "|       2|15-02-2023| 200.0| 2023-02-15|2023|    2|          4|\n",
      "|       3|2023/03/20| 99.99| 2023-03-20|2023|    3|          2|\n",
      "+--------+----------+------+-----------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Анализ продаж по времени\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data\").getOrCreate()\n",
    "\n",
    "# Создаём DataFrame с датами\n",
    "sales_data = [\n",
    "    (1, \"2023-01-15\", 150.0),\n",
    "    (2, \"15-02-2023\", 200.0),  # Нестандартный формат\n",
    "    (3, \"2023/03/20\", 99.99)\n",
    "]\n",
    "df = spark.createDataFrame(sales_data, [\"order_id\", \"order_date\", \"amount\"])\n",
    "\n",
    "# Приводим даты к единому формату (yyyy-MM-dd)\n",
    "df = df.withColumn(\"parsed_date\", \n",
    "      to_date(col(\"order_date\"), \"yyyy-MM-dd\")) \\\n",
    "      .withColumn(\"parsed_date\", \n",
    "      coalesce(col(\"parsed_date\"), \n",
    "               to_date(col(\"order_date\"), \"dd-MM-yyyy\"),\n",
    "               to_date(col(\"order_date\"), \"yyyy/MM/dd\")))\n",
    "\n",
    "# Извлекаем год, месяц и день недели\n",
    "df = df.withColumn(\"year\", year(\"parsed_date\")) \\\n",
    "       .withColumn(\"month\", month(\"parsed_date\")) \\\n",
    "       .withColumn(\"day_of_week\", dayofweek(\"parsed_date\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94b8d88a-c022-4215-9fcf-f60894e4adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+----+-----+-----------+---------------------+\n",
      "|order_id|order_date|amount|parsed_date|year|month|day_of_week|days_since_last_order|\n",
      "+--------+----------+------+-----------+----+-----+-----------+---------------------+\n",
      "|       1|2023-01-15| 150.0| 2023-01-15|2023|    1|          1|                 NULL|\n",
      "|       2|15-02-2023| 200.0| 2023-02-15|2023|    2|          4|                   31|\n",
      "|       3|2023/03/20| 99.99| 2023-03-20|2023|    3|          2|                   33|\n",
      "+--------+----------+------+-----------+----+-----+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Расчёт дней между заказами\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"year\").orderBy(\"parsed_date\")\n",
    "df = df.withColumn(\"days_since_last_order\", \n",
    "      datediff(col(\"parsed_date\"), lag(\"parsed_date\").over(window)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd16501-1450-4a58-90d4-b1b2a8d7eff3",
   "metadata": {},
   "source": [
    "Работа со строками:\n",
    "- Базовые операции: concat(), substring(), trim()\n",
    "- Регулярные выражения: regexp_extract(), regexp_replace()\n",
    "- Проверки: startswith(), endswith(), contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ebe61f4-eefa-4ae6-b58f-2f06446d976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+\n",
      "|user_id|            email|     domain|\n",
      "+-------+-----------------+-----------+\n",
      "|      1|alice@example.com|example.com|\n",
      "|      2|    bob@gmail.com|  gmail.com|\n",
      "+-------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Очистка и анализ текста\n",
    "\n",
    "# Извлечение домена из email\n",
    "users = spark.createDataFrame([\n",
    "    (1, \"alice@example.com\"),\n",
    "    (2, \"bob@gmail.com\")\n",
    "], [\"user_id\", \"email\"])\n",
    "\n",
    "users = users.withColumn(\"domain\", \n",
    "      regexp_extract(col(\"email\"), \"@(.+)$\", 1))\n",
    "\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fd9b00-8d48-43dc-a3ba-f8f405b0f4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|comment_id|              text|        clean_text|\n",
      "+----------+------------------+------------------+\n",
      "|         1|      This is bad!|      This is ***!|\n",
      "|         2|Worst product ever|Worst product ever|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Замена цензурой нецензурных слов\n",
    "comments = spark.createDataFrame([\n",
    "    (1, \"This is bad!\"),\n",
    "    (2, \"Worst product ever\")\n",
    "], [\"comment_id\", \"text\"])\n",
    "\n",
    "bad_words = [\"bad\", \"worst\"]\n",
    "pattern = \"|\".join(bad_words)\n",
    "comments = comments.withColumn(\"clean_text\", \n",
    "      regexp_replace(col(\"text\"), pattern, \"***\"))\n",
    "\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11a19102-e941-4222-877f-975a0784a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+-----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|  surname|name_length|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+-----------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|   Braund|         23|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|  Cumings|         51|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|Heikkinen|         22|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S| Futrelle|         44|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|    Allen|         24|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка CSV с заголовком и выводом схемы\n",
    "df_Titanic = spark.read.csv(\"data/train.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Разделяем \"Surname, Title Name\" и извлекаем фамилию\n",
    "df_Titanic = df_Titanic.withColumn(\"surname\", \n",
    "      split(col(\"Name\"), \",\")[0])\n",
    "\n",
    "# Длина имени\n",
    "df_Titanic = df_Titanic.withColumn(\"name_length\", \n",
    "      length(col(\"Name\")))\n",
    "\n",
    "df_Titanic.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e6b26-7e20-4291-99d3-d47783340d75",
   "metadata": {},
   "source": [
    "## 2.3 Оптимизация (партиционирование, кэширование)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfdb95-ba5e-4c70-8b33-da0a89e151f9",
   "metadata": {},
   "source": [
    "Партиционирование — разделение данных на части (партиции) для параллельной обработки. Большие файлы (например, 100+ ГБ CSV/Parquet). Частые фильтры по определённым колонкам (например, по дате)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b3284-1d16-4ae1-a4ba-6aa9774900ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d7eea-11f1-45de-9381-d79edd33c51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c2d90-0380-4451-ab14-d19567429be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90f5688-a85f-4043-aaee-be5bfadf960f",
   "metadata": {},
   "source": [
    "## 2.4 UDF (пользовательские функции)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e21f63-5441-4d7e-a652-70789cc9aab0",
   "metadata": {},
   "source": [
    "## 2.5 Чтение/запись в разных форматах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c1ea360-3271-4591-b0b6-6e51aead9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26344c85-3639-4369-a143-687d4b2628c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
