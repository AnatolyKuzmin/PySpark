{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c894e5",
   "metadata": {},
   "source": [
    "# 4. Экспертные темы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a9f2f",
   "metadata": {},
   "source": [
    "## 4.1. Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406dd4e4-59c1-4108-9c96-e8a4c210f8db",
   "metadata": {},
   "source": [
    "Delta Lake — это открытый формат хранения данных, который добавляет ACID-транзакции, управление метаданными и другие enterprise-функции к вашим данным в Data Lakes.\n",
    "\n",
    "| Функции | Преимущество |\n",
    "| - | - |\n",
    "| ACID-транзакции | Гарантированная целостность данных при параллельныхзапросах |\n",
    "| Time Travel | Доступ к историческим данным |\n",
    "| Schema Enforcement | Контроль схема при записи |\n",
    "| Upsert/Delete | Поддержка операций MERGE, UPDATE, DELETE |\n",
    "| Оптимизация файлов | Автоматическая компактификация маленьких файлов |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d0910-f282-4d62-adc9-2cdcbd0df4da",
   "metadata": {},
   "source": [
    "Установка `pip install delta-spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8c4c5-c3b5-4f20-aba0-741ac741abd1",
   "metadata": {},
   "source": [
    "Инициализация Spark с Delta\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLake\") \\\n",
    "    .config(\"spark.jars\", \"/path/to/delta-core_2.12.jar\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Создание Delta-таблицы\n",
    "```python\n",
    "df.write.format(\"delta\").save(\"/data/delta/events\")\n",
    "\n",
    "# С явным указанием схемы\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE events (\n",
    "    id LONG,\n",
    "    date STRING,\n",
    "    event_type STRING\n",
    "  ) USING DELTA\n",
    "  LOCATION '/data/delta/events'\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "Чтение данных\n",
    "```python\n",
    "delta_df = spark.read.format(\"delta\").load(\"/data/delta/events\")\n",
    "\n",
    "# Чтение конкретной версии\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", 5) \\\n",
    "  .load(\"/data/delta/events\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4beb47a-c5ca-4f87-befa-6e491cd184e4",
   "metadata": {},
   "source": [
    "#### Time Travel — доступ к истории"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a8cd2-7e63-4870-a7e7-d525dae06752",
   "metadata": {},
   "source": [
    "Просмотр истории изменений\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/data/delta/events\")\n",
    "history = delta_table.history()\n",
    "history.show()\n",
    "```\n",
    "\n",
    "Восстановление данных\n",
    "```python\n",
    "# Восстановление до версии 2\n",
    "delta_table.restoreToVersion(2)\n",
    "\n",
    "# Восстановление до timestamp\n",
    "delta_table.restoreToTimestamp(\"2023-01-01 00:00:00\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9dc074-b15d-46fe-89b7-e5d877a80e14",
   "metadata": {},
   "source": [
    "#### Операции Upsert и Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41a86b-fba2-49ac-af1c-9f7b80681fa5",
   "metadata": {},
   "source": [
    "MERGE (Upsert)\n",
    "```python\n",
    "updates_df = spark.createDataFrame([(1, \"new_data\")], [\"id\", \"data\"])\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    updates_df.alias(\"updates\"),\n",
    "    \"target.id = updates.id\"\n",
    ").whenMatchedUpdate(set={\"data\": \"updates.data\"}) \\\n",
    " .whenNotMatchedInsert(values={\n",
    "    \"id\": \"updates.id\",\n",
    "    \"data\": \"updates.data\"\n",
    "}).execute()\n",
    "```\n",
    "\n",
    "Удаление данных\n",
    "```python\n",
    "delta_table.delete(\"date < '2023-01-01'\")  # Удалить старые записи\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dae66a-ed46-4bcc-8996-fdc63ff1719f",
   "metadata": {},
   "source": [
    "#### Оптимизация Delta-таблиц"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27835c9-af05-447a-9128-e4eca966be21",
   "metadata": {},
   "source": [
    "Компактификация файлов\n",
    "```python\n",
    "delta_table.optimize().executeCompaction()\n",
    "```\n",
    "\n",
    "Z-Ordering (кластеризация)\n",
    "```python\n",
    "delta_table.optimize().executeZOrderBy(\"event_type\")\n",
    "```\n",
    "\n",
    "Вакуумирование (удаление старых версий)\n",
    "```python\n",
    "delta_table.vacuum()                  # Удалить версии старше 7 дней\n",
    "delta_table.vacuum(48)                # Удалить версии старше 48 часов\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c4604-70ba-437e-bbb7-78774443f83d",
   "metadata": {},
   "source": [
    "#### Практический кейс: CDC (Change Data Capture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b2d34-6790-4234-a4ad-d658696825c9",
   "metadata": {},
   "source": [
    "Задача: Реализовать обработку изменений из источника в Delta Lake.\n",
    "```python\n",
    "# 1. Создаём стриминг из Kafka\n",
    "changes = spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"subscribe\", \"updates\") \\\n",
    "  .load()\n",
    "\n",
    "# 2. Парсим JSON-сообщения\n",
    "parsed = changes.select(\n",
    "  from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# 3. Записываем изменения в Delta с MERGE\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    delta_table = DeltaTable.forPath(spark, \"/data/delta/target\")\n",
    "    delta_table.alias(\"t\").merge(\n",
    "        microBatchDF.alias(\"s\"),\n",
    "        \"t.id = s.id\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "\n",
    "parsed.writeStream \\\n",
    "  .foreachBatch(upsert_to_delta) \\\n",
    "  .option(\"checkpointLocation\", \"/checkpoints/delta_cdc\") \\\n",
    "  .start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1fa64e-b29a-49ee-876e-0af2770e56bd",
   "metadata": {},
   "source": [
    "#### Мониторинг и обслуживание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a9c3d-4ee8-4030-a5ea-4f48b54cd9ee",
   "metadata": {},
   "source": [
    "Анализ метаданных\n",
    "```python\n",
    "spark.sql(\"ANALYZE TABLE events COMPUTE STATISTICS\")\n",
    "spark.sql(\"DESCRIBE DETAIL events\").show()\n",
    "```\n",
    "\n",
    "Размеры файлов\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "  SELECT file_size, num_records \n",
    "  FROM delta.`/data/delta/events`.files\n",
    "\"\"\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90baba3",
   "metadata": {},
   "source": [
    "## 4.2. MLlib (машинное обучение)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e7b04-b13b-458e-bf2c-75c73b859691",
   "metadata": {},
   "source": [
    "MLlib — это масштабируемая библиотека машинного обучения Spark, которая предоставляет:\n",
    "- Алгоритмы для классификации, регрессии, кластеризации\n",
    "- Инструменты для обработки признаков (feature engineering)\n",
    "- Конвейеры (Pipelines) для построения ML-пайплайнов\n",
    "- Интеграцию с Python-библиотеками (sklearn, TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa227a-0f43-4482-bcd9-4b764f510db6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Загрузка данных (на примере Titanic)**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MLlib\").getOrCreate()\n",
    "df = spark.read.csv(\"titanic.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Выборка признаков и целевой переменной\n",
    "data = df.select(\n",
    "    \"Survived\",  # Целевая переменная\n",
    "    \"Pclass\", \n",
    "    \"Sex\", \n",
    "    \"Age\", \n",
    "    \"Fare\"\n",
    ").na.drop()  # Удаление пропусков\n",
    "```\n",
    "\n",
    "#### **Обработка категориальных признаков**\n",
    "```python\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, \n",
    "    OneHotEncoder,\n",
    "    VectorAssembler\n",
    ")\n",
    "\n",
    "# Преобразование строк в числовые индексы\n",
    "sex_indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "pclass_indexer = StringIndexer(inputCol=\"Pclass\", outputCol=\"PclassIndex\")\n",
    "\n",
    "# One-Hot Encoding\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"SexIndex\", \"PclassIndex\"],\n",
    "    outputCols=[\"SexVec\", \"PclassVec\"]\n",
    ")\n",
    "\n",
    "# Объединение всех признаков в один вектор\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"PclassVec\", \"SexVec\", \"Age\", \"Fare\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Создание конвейера преобразований**\n",
    "```python\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    sex_indexer,\n",
    "    pclass_indexer,\n",
    "    encoder,\n",
    "    assembler\n",
    "])\n",
    "\n",
    "# Применение пайплайна к данным\n",
    "processed_data = pipeline.fit(data).transform(data)\n",
    "processed_data.show(3)\n",
    "```\n",
    "\n",
    "#### **Разделение на train/test**\n",
    "```python\n",
    "train_data, test_data = processed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Логистическая регрессия**\n",
    "```python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Survived\",\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "model = lr.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"Survived\", \"prediction\").show(5)\n",
    "```\n",
    "\n",
    "#### **Случайный лес**\n",
    "```python\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"Survived\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Метрики качества**\n",
    "```python\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    MulticlassClassificationEvaluator\n",
    ")\n",
    "\n",
    "# AUC-ROC\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Survived\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc:.3f}\")\n",
    "\n",
    "# Accuracy\n",
    "acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Survived\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = acc_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "```\n",
    "\n",
    "#### **Матрица ошибок**\n",
    "```python\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Конвертация в RDD\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"Survived\").rdd\n",
    "\n",
    "# Расчёт метрик\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Cross-Validator**\n",
    "```python\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Сетка параметров\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build())\n",
    "\n",
    "# Кросс-валидация\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_data)\n",
    "best_model = cv_model.bestModel\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Сохранение и загрузка моделей**\n",
    "```python\n",
    "# Сохранение\n",
    "best_model.write().overwrite().save(\"models/best_lr_model\")\n",
    "\n",
    "# Загрузка\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "loaded_model = LogisticRegressionModel.load(\"models/best_lr_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Практическое задание**\n",
    "**Задача**: Постройте модель для предсказания цены домов (регрессия)  \n",
    "**Датасет**: [California Housing](https://www.kaggle.com/datasets/camnugent/california-housing-prices)\n",
    "\n",
    "1. Загрузите данные и обработайте категориальные признаки\n",
    "2. Постройте пайплайн с RandomForestRegressor\n",
    "3. Оцените модель с помощью RMSE и R2\n",
    "4. Подберите оптимальные гиперпараметры\n",
    "\n",
    "```python\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol=\"median_house_value\",\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "# Решение...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Использование sklearn внутри Spark**\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def sklearn_predict(features: pd.Series) -> pd.Series:\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(train_pd[features], train_pd[label])\n",
    "    return pd.Series(model.predict(features))\n",
    "```\n",
    "\n",
    "#### **Использование TensorFlow/PyTorch**\n",
    "```python\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "\n",
    "def train_fn():\n",
    "    import torch\n",
    "    # Код обучения модели\n",
    "    \n",
    "distributor = TorchDistributor(num_processes=2, local_mode=True)\n",
    "distributor.run(train_fn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19516a91",
   "metadata": {},
   "source": [
    "## 4.3. Распределенная обработка (Glue, EMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db99a6-8d9f-4795-bc85-18d7fe0cf7e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **AWS Glue: Серверный ETL-сервис. Ключевые возможности**\n",
    "- **Автоматическое обнаружение схемы** (Schema Discovery)\n",
    "- **Встроенные преобразования** (Transformations)\n",
    "- **Интеграция с каталогом данных** (Glue Data Catalog)\n",
    "- **Серверная архитектура** (нет управления инфраструктурой)\n",
    "\n",
    "#### **Создание Glue Job для обработки данных**\n",
    "```python\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Чтение данных из S3\n",
    "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"your-database\",\n",
    "    table_name=\"raw_data\"\n",
    ")\n",
    "\n",
    "# Преобразование данных\n",
    "def transform(record):\n",
    "    record[\"new_column\"] = record[\"existing_column\"] * 2\n",
    "    return record\n",
    "\n",
    "transformed = Map.apply(frame=datasource, f=transform)\n",
    "\n",
    "# Запись результатов в Parquet\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=transformed,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://output-bucket/processed/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Запуск Glue Job через AWS CLI**\n",
    "```bash\n",
    "aws glue start-job-run --job-name \"your-glue-job\" --arguments='--extra-py-files=\"s3://your-bucket/libs.zip\"'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Amazon EMR: Управляемый Hadoop-кластер. Создание кластера EMR**\n",
    "```bash\n",
    "aws emr create-cluster \\\n",
    "--name \"PySpark Cluster\" \\\n",
    "--release-label emr-6.15.0 \\\n",
    "--applications Name=Spark \\\n",
    "--ec2-attributes KeyName=your-key-pair \\\n",
    "--instance-type m5.xlarge \\\n",
    "--instance-count 3 \\\n",
    "--log-uri s3://your-logs-bucket/ \\\n",
    "--steps Type=Spark,Name=\"Spark Job\",ActionOnFailure=CONTINUE,Args=[--deploy-mode,cluster,--master,yarn,s3://your-bucket/your_script.py] \\\n",
    "--use-default-roles\n",
    "```\n",
    "\n",
    "#### **Пример PySpark-скрипта для EMR**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EMR Processing\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Чтение данных из S3\n",
    "df = spark.read.parquet(\"s3://input-bucket/data/\")\n",
    "\n",
    "# Обработка данных\n",
    "result = df.groupBy(\"category\").count()\n",
    "\n",
    "# Запись результатов\n",
    "result.write.parquet(\"s3://output-bucket/results/\")\n",
    "```\n",
    "\n",
    "#### **Мониторинг EMR**\n",
    "- **EMR Console**: Просмотр состояния кластера\n",
    "- **YARN ResourceManager**: `http://<master-public-dns>:8088`\n",
    "- **Spark History Server**: `http://<master-public-dns>:18080`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Оптимизация производительности. Настройки Glue**\n",
    "```python\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "glueContext.setConf(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "glueContext.setConf(\"spark.default.parallelism\", \"1000\")\n",
    "```\n",
    "\n",
    "#### **Выбор инстансов EMR**\n",
    "| **Тип задачи**       | **Рекомендуемые инстансы**      | **Конфигурация**                  |\n",
    "|----------------------|--------------------------------|----------------------------------|\n",
    "| Память-интенсивная   | r5.2xlarge, r6g.xlarge         | --executor-memory 20G            |\n",
    "| CPU-интенсивная      | c5.2xlarge, c6g.xlarge         | --executor-cores 4               |\n",
    "| Сбалансированная     | m5.2xlarge, m6g.xlarge         | --num-executors 10               |\n",
    "\n",
    "#### **араметры Spark Submit для EMR**\n",
    "```bash\n",
    "spark-submit \\\n",
    "--deploy-mode cluster \\\n",
    "--master yarn \\\n",
    "--executor-memory 20G \\\n",
    "--executor-cores 4 \\\n",
    "--num-executors 10 \\\n",
    "your_script.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Интеграция с другими сервисами AWS. Чтение/запись в DynamoDB**\n",
    "```python\n",
    "# Чтение\n",
    "dyf = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"dynamodb\",\n",
    "    connection_options={\n",
    "        \"dynamodb.input.tableName\": \"your-table\",\n",
    "        \"dynamodb.throughput.read.percent\": \"0.5\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Запись\n",
    "glueContext.write_dynamic_frame_from_options(\n",
    "    frame=dyf,\n",
    "    connection_type=\"dynamodb\",\n",
    "    connection_options={\n",
    "        \"dynamodb.output.tableName\": \"output-table\",\n",
    "        \"dynamodb.throughput.write.percent\": \"0.5\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Обработка потоковых данных (Kinesis)**\n",
    "```python\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"your-stream\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .load()\n",
    "\n",
    "query = streaming_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"s3://output-bucket/streaming/\") \\\n",
    "    .option(\"checkpointLocation\", \"s3://checkpoint-bucket/\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Практическое задание**\n",
    "**Задача**: Разверните ETL-пайплайн для обработки логов веб-сайта  \n",
    "**Данные**: Логи в S3 в формате JSON (1+ TB данных)  \n",
    "**Требования**:\n",
    "1. Используйте AWS Glue для очистки и трансформации данных\n",
    "2. Разверните EMR-кластер для агрегации данных\n",
    "3. Сохраните результаты в Parquet-формате\n",
    "4. Настройте автоматическое масштабирование кластера\n",
    "\n",
    "**Решение**:\n",
    "```python\n",
    "# Glue Job (preprocessing.py)\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "logs = glueContext.create_dynamic_frame.from_options(\n",
    "    \"s3\",\n",
    "    {\"paths\": [\"s3://input-logs/\"]},\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "# Фильтрация и преобразование\n",
    "cleaned = logs.filter(lambda r: r[\"status\"] == 200)\n",
    "transformed = cleaned.apply_mapping([\n",
    "    (\"timestamp\", \"string\", \"event_time\", \"timestamp\"),\n",
    "    (\"user_id\", \"string\", \"user_id\", \"string\")\n",
    "])\n",
    "\n",
    "# Запись промежуточных данных\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=transformed,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://processed-logs/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Лучшие практики**\n",
    "1. **Glue**:\n",
    "   - Используйте **закладки (bookmarks)** для инкрементальной обработки\n",
    "   - Оптимизируйте **DPU (Data Processing Units)** для стоимости\n",
    "\n",
    "2. **EMR**:\n",
    "   - Включайте **EMR Managed Scaling** для автоматического масштабирования\n",
    "   - Используйте **Spot Instances** для экономии (до 90%)\n",
    "\n",
    "3. **Данные**:\n",
    "   - Разделяйте данные по **партициям** (дата, регион)\n",
    "   - Используйте **колоночные форматы** (Parquet, ORC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb31e6d",
   "metadata": {},
   "source": [
    "## 4.4. Отладка и логирование  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda52dd-cfe2-4b0b-8d21-914ae7cd9dfc",
   "metadata": {},
   "source": [
    "#### **Настройка логирования в PySpark. Базовая конфигурация**\n",
    "```python\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Настройка уровня логирования\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"WARN\")  # Уровни: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "#### **Кастомизация формата логов**\n",
    "Добавьте в `$SPARK_HOME/conf/log4j2.properties`:\n",
    "```properties\n",
    "name=SparkConfig\n",
    "appender.console.type=Console\n",
    "appender.console.name=STDOUT\n",
    "appender.console.layout.type=PatternLayout\n",
    "appender.console.layout.pattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
    "\n",
    "logger.spark.name=org.apache.spark\n",
    "logger.spark.level=warn\n",
    "\n",
    "rootLogger.level=warn\n",
    "rootLogger.appenderRefs=stdout\n",
    "rootLogger.appenderRef.stdout.ref=STDOUT\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Инструменты отладки. Spark UI**\n",
    "Доступен по адресу `http://<driver-node>:4040`:\n",
    "- **Jobs**: Статус и длительность заданий\n",
    "- **Stages**: Детализация по стадиям выполнения\n",
    "- **Storage**: Кэшированные RDD/DataFrame\n",
    "- **Environment**: Конфигурация Spark\n",
    "\n",
    "#### **Анализ плана выполнения**\n",
    "```python\n",
    "# Объяснение физического плана\n",
    "df.explain(mode=\"extended\")\n",
    "\n",
    "# Сохранение плана выполнения в файл\n",
    "with open(\"query_plan.txt\", \"w\") as f:\n",
    "    f.write(df._jdf.queryExecution().toString())\n",
    "```\n",
    "\n",
    "#### **Мониторинг через API**\n",
    "```python\n",
    "# Получение информации о заданиях\n",
    "status_tracker = sc.statusTracker()\n",
    "for job_id in status_tracker.getJobIdsForGroup():\n",
    "    job_info = status_tracker.getJobInfo(job_id)\n",
    "    print(f\"Job {job_id}: {job_info.status}\")\n",
    "\n",
    "# Метрики выполнения\n",
    "print(sc.uiWebUrl)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Отладка распределённых приложений. Логирование внутри преобразований**\n",
    "```python\n",
    "def debug_transform(partition):\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    for row in partition:\n",
    "        logger.info(f\"Processing row: {row}\")\n",
    "        yield row\n",
    "\n",
    "rdd.mapPartitions(debug_transform).count()\n",
    "```\n",
    "\n",
    "#### **Отлов исключений**\n",
    "```python\n",
    "def safe_transform(row):\n",
    "    try:\n",
    "        # Ваша логика обработки\n",
    "        return processed_row\n",
    "    except Exception as e:\n",
    "        import sys\n",
    "        print(f\"Error processing row {row}: {e}\", file=sys.stderr)\n",
    "        return None  # или специальное значение для обработки ошибок\n",
    "\n",
    "clean_rdd = rdd.map(safe_transform).filter(lambda x: x is not None)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Анализ распространённых ошибок. OOM (Out of Memory)**\n",
    "**Симптомы**:\n",
    "- `java.lang.OutOfMemoryError`\n",
    "- Зависание задач\n",
    "\n",
    "**Решение**:\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "#### **Data Skew (Перекос данных)**\n",
    "**Симптомы**:\n",
    "- Несколько задач выполняются значительно дольше других\n",
    "- Медленные JOIN-операции\n",
    "\n",
    "**Решение**:\n",
    "```python\n",
    "# Сольтинг (salting)\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "df = df.withColumn(\"salt\", (rand() * 100).cast(\"int\"))\n",
    "df.groupBy(\"key\", \"salt\").count()\n",
    "```\n",
    "\n",
    "#### **Проблемы с сериализацией**\n",
    "**Симптомы**:\n",
    "- `PicklingError` или `SerializationException`\n",
    "\n",
    "**Решение**:\n",
    "```python\n",
    "# Убедитесь, что все функции и объекты сериализуемы\n",
    "def external_function(x):\n",
    "    return x * 2\n",
    "\n",
    "# Неправильно:\n",
    "rdd.map(lambda x: some_external_function(x))\n",
    "\n",
    "# Правильно:\n",
    "rdd.map(external_function)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Продвинутые техники. Логирование в распределённом режиме**\n",
    "```python\n",
    "def log_partition(iterator):\n",
    "    import socket\n",
    "    host = socket.gethostname()\n",
    "    print(f\"Processing on {host}\")\n",
    "    yield sum(1 for _ in iterator)\n",
    "\n",
    "count = rdd.mapPartitions(log_partition).sum()\n",
    "```\n",
    "\n",
    "#### **Интеграция с Sentry/ELK**\n",
    "```python\n",
    "from py4j.java_gateway import JavaGateway\n",
    "gateway = JavaGateway()\n",
    "\n",
    "def log_to_elk(message):\n",
    "    elk_logger = gateway.jvm.org.apache.log4j.Logger.getLogger(\"ELK\")\n",
    "    elk_logger.info(message)\n",
    "\n",
    "rdd.foreach(lambda x: log_to_elk(f\"Processed: {x}\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Практическое задание**\n",
    "**Задача**: Реализовать отладочный пайплайн для ETL-процесса  \n",
    "**Требования**:\n",
    "1. Логировать количество обработанных строк в каждом партицировании\n",
    "2. Собирать метрики времени выполнения операций\n",
    "3. Реализовать обработку ошибок с сохранением \"битых\" записей\n",
    "4. Генерировать отчёт о качестве данных\n",
    "\n",
    "**Решение**:\n",
    "```python\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def debug_pipeline(partition):\n",
    "    metrics = {\n",
    "        \"start_time\": datetime.now().isoformat(),\n",
    "        \"processed\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"bad_rows\": []\n",
    "    }\n",
    "    \n",
    "    for row in partition:\n",
    "        try:\n",
    "            # Ваша обработка\n",
    "            processed_row = transform(row)\n",
    "            metrics[\"processed\"] += 1\n",
    "            yield processed_row\n",
    "        except Exception as e:\n",
    "            metrics[\"errors\"] += 1\n",
    "            metrics[\"bad_rows\"].append({\n",
    "                \"row\": str(row),\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    metrics[\"end_time\"] = datetime.now().isoformat()\n",
    "    with open(f\"/tmp/metrics_{datetime.now().timestamp()}.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "# Применение\n",
    "result_rdd = input_rdd.mapPartitions(debug_pipeline)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Лучшие практики**\n",
    "1. **Логируйте контекст**: Добавляйте ID задания, время, хост\n",
    "2. **Разделяйте логи**: Отдельные логи для ошибок, предупреждений, информации\n",
    "3. **Используйте структурированный формат**: JSON для логов\n",
    "4. **Мониторьте ключевые метрики**:\n",
    "   - Время выполнения задач\n",
    "   - Использование памяти\n",
    "   - Количество ошибок\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
