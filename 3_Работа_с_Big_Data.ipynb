{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e6ee05-e059-483a-b852-1906627036a6",
   "metadata": {},
   "source": [
    "# 3. Работа с Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7391b90-7358-4a3d-b1e3-53886385cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'  # Обход проверки пользователя\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba58bc2-63a0-47f3-9b0b-57068a6056c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3.1. Spark SQL (каталог, метаданные)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce2e8b-a799-4591-a1d4-c176dd1e9b5c",
   "metadata": {},
   "source": [
    "Каталог (Catalog) — это абстракция в Spark SQL, которая предоставляет API для:\n",
    "- Управления базами данных (databases)\n",
    "- Работы с таблицами (tables)\n",
    "- Доступа к представлениям (views)\n",
    "- Хранения метаданных (схем, типов данных, партиций)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb18116-56e7-41d1-ac98-ca4854eeeb79",
   "metadata": {},
   "source": [
    "Встроенный каталог vs Hive Metastore\n",
    "| Особенность | Встроенный каталог | Hive Metastore |\n",
    "|- |- |- |\n",
    "| Где хранятся метаданные? | В памяти (исчезают после завершения сессии) | Во внешней БД (MySQL, PostgreSQL) |\n",
    "| Когда использовать? | Для временных таблиц | Для постоянных таблиц в продакшене |\n",
    "| Доступ из других приложений | Нет| Да (через Hive, Impala и др.) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b2d09-4b7e-4dad-bf76-71bca44ebe4f",
   "metadata": {},
   "source": [
    "Работа с каталогом через SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373ca66-f5db-4361-927b-d21f6d9f48cb",
   "metadata": {},
   "source": [
    "Показать все базы данных\n",
    "`spark.catalog.listDatabases()`  \n",
    "Показать все таблицы в текущей БД\n",
    "`spark.catalog.listTables()`  \n",
    "Показать все колонки таблицы\n",
    "`spark.catalog.listColumns(df_Titanic)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8cb221-2ad3-45d3-8094-06059477af2f",
   "metadata": {},
   "source": [
    "Создание и управление базами данных\n",
    "Создать новую БД\n",
    "`spark.sql(\"CREATE DATABASE IF NOT EXISTS my_db\")`  \n",
    "Переключиться на БД\n",
    "`spark.catalog.setCurrentDatabase(\"my_db\")`  \n",
    "Удалить БД (осторожно!)\n",
    "`spark.sql(\"DROP DATABASE IF EXISTS my_db CASCADE\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb28f13-4a62-4492-9c56-b2913ede84d6",
   "metadata": {},
   "source": [
    "Временные vs постоянные таблицы\n",
    "\n",
    "| Тип | Временная таблица | Постоянная таблица |\n",
    "|- |- |- |\n",
    "| Видимость | Только в текущей сессии Spark | Доступна всем сессиям |\n",
    "| Хранение метаданных | В памяти | В Hive Metastore |\n",
    "| Хранение данных | Зависит от источника| Сохраняется на HDFS/S3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d6285-2b9d-45f5-a8c9-6f90330e7287",
   "metadata": {},
   "source": [
    "Временная таблица (исчезнет после завершения сессии)\n",
    "`df.createOrReplaceTempView(\"temp_titanic\")`  \n",
    "Постоянная таблица (сохранится в каталоге Hive)\n",
    "`df.write.saveAsTable(\"perm_titanic\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01527e1-9d7c-44d6-a724-4b5103347e58",
   "metadata": {},
   "source": [
    "***VIEW*** — это виртуальная таблица (запрос не выполняется, пока не вызвана).\n",
    "***TABLE*** — материализованные данные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61725a-a830-43b6-b175-a9813bee8278",
   "metadata": {},
   "source": [
    "```\n",
    "# Временное представление\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW adult_passengers AS\n",
    "    SELECT * FROM titanic WHERE Age >= 18\n",
    "\"\"\")\n",
    "\n",
    "# Постоянное представление (требует Hive)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW perm_adult_passengers AS\n",
    "    SELECT * FROM titanic WHERE Age >= 18\n",
    "\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f0c5f-f664-4726-8a79-5034714e4b8a",
   "metadata": {},
   "source": [
    "Интеграция с Hive Metastore. Чтобы Spark мог работать с Hive Metastore, добавьте в spark-defaults.conf\n",
    "```\n",
    "spark.sql.catalogImplementation=hive\n",
    "spark.hadoop.hive.metastore.uris=thrift://metastore-host:9083\n",
    "```\n",
    "\n",
    "```\n",
    "# Чтение таблицы из Hive\n",
    "hive_df = spark.sql(\"SELECT * FROM hive_db.hive_table\")\n",
    "\n",
    "# Запись данных в Hive\n",
    "df.write.saveAsTable(\"hive_db.new_table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe87172-f728-4f1d-bc6f-2591081d0c94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3.2. Structured Streaming (основы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011c4cd-cdff-4e64-87fe-3d0deda2112e",
   "metadata": {},
   "source": [
    "Structured Streaming — это масштабируемая и отказоустойчивая система потоковой обработки данных, построенная на движке Spark SQL.\n",
    "\n",
    "Принцип работы:\n",
    "- Бесконечный DataFrame: Потоковые данные представляются как \"бесконечно растущая\" таблица\n",
    "- Микропакетная обработка: Данные обрабатываются небольшими порциями (микропакетами)\n",
    "- Exactly-once семантика: Гарантируется однократная обработка каждого события\n",
    "\n",
    "**Источники**(Input) - Kafka, Файлы (CSV, Parquet), Сокеты (тестирование) и **приемники**(Output) - Консоль (debug), Файлы, Kafka, Базы данных (JDBC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53265ce-aabd-430c-8f94-0b124f820a12",
   "metadata": {},
   "source": [
    "Создание потокового приложения. Чтение данных из сокета (для тестирования)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce2cfd-4098-4d94-8ef3-b2659f28fa1b",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Создаем потоковый DataFrame\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Простая трансформация - подсчет длины строк\n",
    "lengths = lines.select(length(col(\"value\")).alias(\"length\"))\n",
    "\n",
    "# Вывод в консоль\n",
    "query = lengths \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "Как запустить:\n",
    "- Откройте терминал и запустите Netcat: `nc -lk 9999`\n",
    "- Введите несколько строк текста\n",
    "- В консоли Spark появятся результаты обработки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf61d40-f080-4f57-8c4b-f7c8ac249852",
   "metadata": {},
   "source": [
    "#### Обработка потоковых данных из файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1f706-3d62-4ad0-8cb1-8e2e455c4825",
   "metadata": {},
   "source": [
    "Мониторинг папки с новыми файлами\n",
    "```\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True)\n",
    "])\n",
    "\n",
    "stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\  # Обрабатывать по 1 файлу за раз\n",
    "    .csv(\"path/to/input_folder\")\n",
    "\n",
    "# Агрегация по продуктам\n",
    "product_counts = stream_df.groupBy(\"product\").count()\n",
    "\n",
    "query = product_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**maxFilesPerTrigger** - контроль скорости обработки  \n",
    "**latestFirst** - обрабатывать сначала новые файлы  \n",
    "**cleanSource** - удалять обработанные файлы  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579849b-2719-4a33-aebd-f3eab49a27ce",
   "metadata": {},
   "source": [
    "#### Работа с временными окнами (Window Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db51fc-63cf-4a3b-998a-d9b5343644ea",
   "metadata": {},
   "source": [
    "Агрегация по временным окнам\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "windowed_counts = stream_df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\  # Водяной знак\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"5 minutes\"),  # Окно 5 минут\n",
    "        \"product\"\n",
    "    ) \\\n",
    "    .agg(avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "query = windowed_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "Водяной знак (Watermark): Позволяет обрабатывать задержавшиеся данные\n",
    "\n",
    "Типы окон:\n",
    "- Tumbling (фиксированные без перекрытия)\n",
    "- Sliding (перекрывающиеся)\n",
    "- Session (по активности)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b040dd-a660-4721-8ca2-4278711ec51c",
   "metadata": {},
   "source": [
    "#### Интеграция с Kafka "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb24d9-0e17-4ef7-ad48-6c887ac7611c",
   "metadata": {},
   "source": [
    "Чтение данных из Kafka\n",
    "```\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "    .option(\"subscribe\", \"topic1\") \\\n",
    "    .load()\n",
    "\n",
    "# Парсинг JSON-сообщений\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_schema = StructType([...])  # Определите схему JSON\n",
    "parsed_df = df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "```\n",
    "\n",
    "Запись в Kafka\n",
    "```\n",
    "query = processed_df \\\n",
    "    .selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"output_topic\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "    .start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bf43a-a6fb-4b86-8a9b-2e9b4907616f",
   "metadata": {},
   "source": [
    "Управление состоянием и чекпоинты. Настройка чекпоинтов.\n",
    "```\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"output_path\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir\") \\  # Обязательно!\n",
    "    .start()\n",
    "```\n",
    "\n",
    "Восстановление после сбоев  \n",
    "Точный контроль над смещениями (offsets)  \n",
    "Управление состоянием агрегаций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a56e6-1197-4a82-b536-bd27b2638be0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3.3. Оптимизация запросов (Explain, Tungsten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29166439-7e38-4af6-a0b7-d883fc302172",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3.4. Работа с GraphFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e0ca09-595b-4c44-b6b6-433f30f384e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3.5. Интеграция с Python (Pandas, NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e73e14-e327-41af-afd0-c3cdbb0766a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
