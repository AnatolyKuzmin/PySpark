{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f144fb57-b165-4b55-b118-91fbf186dbca",
   "metadata": {},
   "source": [
    "# 1. Основы PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6fd6e-4316-488a-b399-e072717460ad",
   "metadata": {},
   "source": [
    "**Всегда закрывайте сессию после работы**\n",
    "```\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd723522-9d3e-4f8e-b099-5656a15b755a",
   "metadata": {},
   "source": [
    "Установите Java (обязательно!). Скачайте Amazon Corretto 11 (или OpenJDK 11).  \n",
    "```\n",
    "# Проверка (в PowerShell):\n",
    "java -version\n",
    "```\n",
    "Установите Python 3.11. Скачайте с официального сайта. При установке отметьте \"Add Python to PATH\".  \n",
    "```\n",
    "python --version\n",
    "pip --version\n",
    "```\n",
    "Установите Hadoop и Spark. Скачайте Spark 3.5.x (предпочтительно) и Hadoop 3.3. Распакуйте архив в C:\\spark (или другой путь без пробелов). Скачайте winutils.exe для Hadoop в папку C:\\hadoop\\bin. Добавьте переменные среды:\n",
    "```\n",
    "[System.Environment]::SetEnvironmentVariable(\"HADOOP_HOME\", \"C:\\hadoop\", \"Machine\")\n",
    "[System.Environment]::SetEnvironmentVariable(\"SPARK_HOME\", \"C:\\spark\", \"Machine\")\n",
    "[System.Environment]::SetEnvironmentVariable(\"PATH\", \"$env:PATH;C:\\spark\\bin;C:\\hadoop\\bin\", \"Machine\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61767cb0-7665-4058-8690-950b29c46303",
   "metadata": {},
   "source": [
    "Установите PySpark и зависимости\n",
    "```\n",
    "pip install pyspark pandas jupyter pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61879ce-7486-4c23-9077-1981728a1780",
   "metadata": {},
   "source": [
    "Проверка установки. Тест PySpark в Jupyter\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "В ноутбуке выполните\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()\n",
    "```\n",
    "Вывод\n",
    "\n",
    "| id| name|\n",
    "| -- | -- |\n",
    "|  1|Alice|\n",
    "|  2|  Bob|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2723727-466e-4578-83b5-e9ac4066fd38",
   "metadata": {},
   "source": [
    "**Используемые датасеты**  \n",
    "[Titanic](https://www.kaggle.com/competitions/titanic/data?select=train.csv \"CSV\")  \n",
    "[Amazon Sales Data](https://www.kaggle.com/datasets/karkavelrajaj/amazon-sales-dataset \"JSON\")  \n",
    "[COVID-19 Dataset](https://www.kaggle.com/datasets/imdevskp/corona-virus-report \"Parquet\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e4a64b-61b2-45cd-a9ba-7285758bf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'  # Обход проверки пользователя\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141d635f-e7e6-4335-96af-c11172ee1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d4d85-606d-4747-8eb4-b0c3357cdd7d",
   "metadata": {},
   "source": [
    "## 1.1. Введение в Spark и RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66d445-4c6d-46b6-89c4-38e8e4f16c4c",
   "metadata": {},
   "source": [
    "PySpark — это Python API для Apache Spark — высокопроизводительной распределённой платформы для обработки больших данных. Позволяет писать параллельные программы, которые работают на кластерах, но при этом использовать знакомый Python. Spark работает с большими объёмами данных, поддерживает SQL, машинное обучение, потоковую обработку. Jupyter Notebook — удобный инструмент для интерактивного написания и запуска кода, идеально подходит для обучения и прототипирования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9718d-52f5-4478-acd8-7446dc4eb836",
   "metadata": {},
   "source": [
    "RDD(Resilient Distributed Dataset) - базовая структура данных в Spark. **Неизменяемая:** После создания нельзя изменить, только преобразовать. **Распределённая:** Данные разделены на части (партиции) и обрабатываются параллельно.  \n",
    "SparkSession – точка входа для работы с Spark (аналог SparkContext в старых версиях)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a75df-0bfc-4b26-a398-c7be755904d2",
   "metadata": {},
   "source": [
    "Ключевые характеристики RDD:\n",
    "- Resilient (Устойчивость). RDD автоматически восстанавливается при сбоях благодаря lineage (цепочке преобразований). Если часть данных теряется, Spark пересчитывает её заново на основе исходных данных и применённых операций.\n",
    "- Distributed (Распределённость). Данные разбиваются на партиции (partitions) и распределяются по узлам кластера. Обработка происходит параллельно.\n",
    "- Dataset (Набор данных). Может содержать данные любого типа (числа, строки, объекты и т. д.). Поддерживает операции: map, filter, reduce, join и др.\n",
    "\n",
    "Как создаются RDD?  \n",
    "```\n",
    "# Из внешних данных (HDFS, S3, локальная файловая система):\n",
    "rdd = sc.textFile(\"hdfs://path/to/file.txt\")\n",
    "\n",
    "# Из коллекций в памяти (например, список Python):\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Преобразованием другого RDD (например, map, filter):\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b567348-c843-4ef0-8edd-0fab93c5767f",
   "metadata": {},
   "source": [
    "### Пример:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b059f-9ea1-44e5-acfa-4c5febe10f60",
   "metadata": {},
   "source": [
    "Создание RDD из списка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f75fe2-a788-4e66-a557-781229d7604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD Basics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Создание RDD из списка чисел\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Трансформация: умножение каждого элемента на 2\n",
    "rdd_doubled = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Действие: сбор результатов\n",
    "print(rdd_doubled.collect())  # Вывод: [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb19b1-8477-4899-865e-0c5c1e5f937c",
   "metadata": {},
   "source": [
    "Фильтрация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb674fb2-e085-4cbd-bb42-4ff47db84e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Фильтрация чётных чисел\n",
    "rdd_even = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(rdd_even.collect())  # Вывод: [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d7116-dca7-43e3-9ad8-42d2ecad13a7",
   "metadata": {},
   "source": [
    "Работа с текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39b1715-092b-4109-b5f2-f0545e3877aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Hello': 3, 'Spark': 1, 'Python': 1, 'World': 1})\n"
     ]
    }
   ],
   "source": [
    "text_data = [\"Hello Spark\", \"Hello Python\", \"Hello World\"]\n",
    "text_rdd = spark.sparkContext.parallelize(text_data)\n",
    "\n",
    "# Разбиваем строки на слова\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Подсчёт слов\n",
    "word_counts = words_rdd.countByValue()\n",
    "print(word_counts)  # Вывод: {'Hello': 3, 'Spark': 1, 'Python': 1, 'World': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6135ff-0229-4983-93d4-6b38fd44c3a8",
   "metadata": {},
   "source": [
    "## 1.2. DataFrames (основы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf53a31-8581-4775-b133-d1a676fcba57",
   "metadata": {},
   "source": [
    "PySpark DataFrame - это распределенная коллекция данных, организованная в именованные столбцы, предоставляемая Apache Spark для работы с большими данными в Python. Это аналог pandas DataFrame, но предназначенный для работы в распределенной среде.\n",
    "\n",
    "**Основные характеристики**. *Распределенная структура*: Данные разделены между узлами кластера. *Неизменяемость*: DataFrame иммутабельны, все операции создают новые DataFrame. *Ленивые вычисления*: Операции выполняются только при вызове действия (action). *Оптимизация*: Использует Catalyst Optimizer для оптимизации запросов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cee046-d0bf-4a78-ae97-160361fbc7db",
   "metadata": {},
   "source": [
    "Отличие от RDD:\n",
    "- Оптимизирован для SQL-подобных операций.\n",
    "- Имеет схему (типы данных колонок).\n",
    "\n",
    "Основные методы:\n",
    "- show() – вывод данных.\n",
    "- printSchema() – вывод структуры.\n",
    "- select(), filter(), groupBy(), agg()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e0e3f-cdf8-4b21-a1ad-738f56cf9e0b",
   "metadata": {},
   "source": [
    "### Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921c603-628a-4cc6-af57-f08abcd36bc5",
   "metadata": {},
   "source": [
    "Воспользуемся датасет Titanic (файл train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0c1aae-6930-4102-b6f0-ceaf506286c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Загрузка CSV с заголовком и выводом схемы\n",
    "df_Titanic = spark.read.csv(\"data/train.csv\", header=True, inferSchema=True)\n",
    "df_Titanic.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbbae4-d6fa-4447-be7f-1741ae08d0a7",
   "metadata": {},
   "source": [
    "Исследование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffae4e68-837d-4728-8fb3-d90af2e90584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------------+-----------------+\n",
      "|summary|               Age|             Fare|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|               714|              891|\n",
      "|   mean| 29.69911764705882| 32.2042079685746|\n",
      "| stddev|14.526497332334035|49.69342859718089|\n",
      "|    min|              0.42|              0.0|\n",
      "|    max|              80.0|         512.3292|\n",
      "+-------+------------------+-----------------+\n",
      "\n",
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Показать первые 5 строк\n",
    "df_Titanic.show(5)\n",
    "\n",
    "# Статистика по числовым колонкам\n",
    "df_Titanic.describe([\"Age\", \"Fare\"]).show()\n",
    "\n",
    "# Количество выживших и погибших\n",
    "df_Titanic.groupBy(\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0b766-2361-4724-aedb-8886fcc89595",
   "metadata": {},
   "source": [
    "Фильтрация и агрегация. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6dea02c-afc4-4205-8ccb-2c893a0ba675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------+\n",
      "|Pclass|           AvgFare| MaxFare|\n",
      "+------+------------------+--------+\n",
      "|     1| 84.15468749999992|512.3292|\n",
      "|     3|13.675550101832997|   69.55|\n",
      "|     2| 20.66218315217391|    73.5|\n",
      "+------+------------------+--------+\n",
      "\n",
      "+-----------------+\n",
      "|         avg(Age)|\n",
      "+-----------------+\n",
      "|28.84771573604061|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, max  # Импортируем нужные функции\n",
    "\n",
    "# Стоимость билета по классам\n",
    "df_Titanic.groupBy(\"Pclass\").agg(avg(\"Fare\").alias(\"AvgFare\"), max(\"Fare\").alias(\"MaxFare\")).show()\n",
    "\n",
    "# Средний возраст выживших женщин.\n",
    "df_Titanic.filter((df_Titanic.Sex == \"female\") & (df_Titanic.Survived == 1)).select(avg(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7689915f-5e50-4ecc-900f-abaf40bcdd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Pclass|count|\n",
      "+------+-----+\n",
      "|     1|  216|\n",
      "|     3|  491|\n",
      "|     2|  184|\n",
      "+------+-----+\n",
      "\n",
      "+-------+-------+\n",
      "|min_age|min_age|\n",
      "+-------+-------+\n",
      "|   0.42|   80.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min  # Импортируем нужные функции\n",
    "\n",
    "# количество пассажиров в каждом классе (Pclass).\n",
    "df_Titanic.groupBy(\"Pclass\").count().show()\n",
    "\n",
    "# минимальный и максимальный возраст пассажиров\n",
    "df_Titanic.agg(min(\"Age\").alias(\"min_age\"), max(\"Age\").alias(\"min_age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be74fe-0b8d-495e-8412-36bda0734ae5",
   "metadata": {},
   "source": [
    "###  JSON в PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95f7ff-e6ad-4299-9846-869b69d8d7a0",
   "metadata": {},
   "source": [
    "JSON – полуструктурированный формат, поддерживающий вложенные поля. Spark автоматически определяет схему, но для сложных структур её лучше задавать вручную. Вложенные поля обрабатываются через . (например, `user.address.city`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81c72d-c725-405a-a37c-597f925cf9e6",
   "metadata": {},
   "source": [
    "Функции для работы с JSON:\n",
    "- from_json() - Преобразует JSON-строку в структуру.\n",
    "- to_json() - Преобразует структуру в JSON-строку.\n",
    "- json_tuple() - Извлекает элементы из JSON-строки.\n",
    "- get_json_object() - Извлекает элемент по JSON-пути."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15083e-e2d9-4afc-844c-23229d66cfe1",
   "metadata": {},
   "source": [
    "## 1.3. Базовые операции с DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a6291-3ccc-43d5-80f5-5220f1ea1edf",
   "metadata": {},
   "source": [
    "Очистка данных. Для того чтобы убрать пропуски (NA/NULL) используют `df.na.fill()` – заполнение пропусков. `df.na.drop()` – удаление строк с пропусками. Дубликаты, `df.dropDuplicates()` – удаление полных дубликатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f134c23-3496-4e81-81b8-eff4e253be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|   Fare|  Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-------+--------+\n",
      "|        499|       0|     1|Allison, Mrs. Hud...|female|25.0|    1|    2|  113781| 151.55|C22 C26|       S|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1| PP 9549|   16.7|     G6|       S|\n",
      "|        701|       1|     1|Astor, Mrs. John ...|female|18.0|    1|    0|PC 17757|227.525|C62 C64|       C|\n",
      "|         55|       0|     1|Ostby, Mr. Engelh...|  male|65.0|    0|    1|  113509|61.9792|    B30|       C|\n",
      "|        332|       0|     1| Partner, Mr. Austen|  male|45.5|    0|    0|  113043|   28.5|   C124|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обработаем пропуски в колонке Age train.csv - df_Titanic\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Заполнение пропусков в Age медианным значением\n",
    "median_age = df_Titanic.approxQuantile(\"Age\", [0.5], 0.01)[0]\n",
    "df_filled = df_Titanic.na.fill({\"Age\": median_age})\n",
    "\n",
    "# Удаление дубликатов (если есть)\n",
    "df_clean = df_filled.dropDuplicates()\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce289bb0-3b7c-4804-b964-5e361fe8bb8e",
   "metadata": {},
   "source": [
    "Для сложных условий (например, удаление дубликатов по конкретным колонкам)  \n",
    "`df.dropDuplicates([\"Name\", \"Pclass\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ab2f1-f14b-493e-ad40-65689f1afd67",
   "metadata": {},
   "source": [
    "Объединение таблиц (JOIN). Типы JOIN: inner (по умолчанию), left, right, full, cross.\n",
    "\n",
    "- INNER JOIN (по умолчанию) - возвращает только строки, где есть совпадения в обеих таблицах\n",
    "- LEFT JOIN (LEFT OUTER JOIN) - возвращает все строки из левой таблицы и совпадающие из правой\n",
    "- RIGHT JOIN (RIGHT OUTER JOIN) - возвращает все строки из правой таблицы и совпадающие из левой\n",
    "- FULL JOIN (FULL OUTER JOIN) - возвращает все строки из обеих таблиц\n",
    "- CROSS JOIN - декартово произведение всех строк обеих таблиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7bb8cac-9693-4690-bc90-79daa895b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+---+----------+-----+\n",
      "|passenger_id|   name|age|ticket_num|price|\n",
      "+------------+-------+---+----------+-----+\n",
      "|           1|  Alice| 28|      A123|150.0|\n",
      "|           2|    Bob| 22|      B456|200.0|\n",
      "|           3|Charlie| 30|      NULL| NULL|\n",
      "+------------+-------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Таблица 1: Пассажиры\n",
    "passengers = spark.createDataFrame([\n",
    "    (1, \"Alice\", 28),\n",
    "    (2, \"Bob\", 22),\n",
    "    (3, \"Charlie\", 30)\n",
    "], [\"passenger_id\", \"name\", \"age\"])\n",
    "\n",
    "# Таблица 2: Билеты\n",
    "tickets = spark.createDataFrame([\n",
    "    (1, \"A123\", 150.0),\n",
    "    (2, \"B456\", 200.0),\n",
    "    (4, \"C789\", 100.0)\n",
    "], [\"passenger_id\", \"ticket_num\", \"price\"])\n",
    "\n",
    "# LEFT JOIN (все пассажиры, даже без билетов)\n",
    "joined_df = passengers.join(tickets, \"passenger_id\", \"left\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea9453-5f43-4cf1-805d-cfd20097b410",
   "metadata": {},
   "source": [
    "**Партиционирование** уменьшает время обработки больших данных.  \n",
    "`df_repartitioned = df.repartition(4, \"Pclass\")  # 4 партиции по колонке Pclass`\n",
    "\n",
    "**Кэширование** ускоряет повторные вычисления  \n",
    "`df.cache()  # Данные сохраняются в памяти`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ca272-bb80-4848-a66a-2adf0eb822fb",
   "metadata": {},
   "source": [
    "## 1.4. SQL-синтаксис в PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed6b07-3413-46ff-839a-8f6fc78fd302",
   "metadata": {},
   "source": [
    "Создание временных представлений. **TempView** – виртуальная таблица, доступная только в текущей сессии Spark. **GlobalTempView** – виртуальная таблица, доступная во всех сессиях (используется реже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9aab565-9740-46f4-9326-9713d93b8e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |  titanic|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание временного представления\n",
    "df_Titanic.createOrReplaceTempView(\"titanic\")\n",
    "\n",
    "# Проверка\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1451b61e-3a08-406c-a714-37bc5da3e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------+\n",
      "|                Name| Age|Pclass|\n",
      "+--------------------+----+------+\n",
      "|Barkworth, Mr. Al...|80.0|     1|\n",
      "|Goldschmidt, Mr. ...|71.0|     1|\n",
      "|Artagaveytia, Mr....|71.0|     1|\n",
      "|Crosby, Capt. Edw...|70.0|     1|\n",
      "|Millet, Mr. Franc...|65.0|     1|\n",
      "+--------------------+----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+------------------+-----+\n",
      "|Pclass|           avg_age|count|\n",
      "+------+------------------+-----+\n",
      "|     3|20.646117647058823|  119|\n",
      "|     2| 25.90156626506024|   87|\n",
      "|     1| 35.36819672131148|  136|\n",
      "+------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Базовые SQL-запросы\n",
    "\n",
    "# Запрос через SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT Name, Age, Pclass \n",
    "    FROM titanic \n",
    "    WHERE Age > 30 AND Pclass = 1\n",
    "    ORDER BY Age DESC\n",
    "\"\"\")\n",
    "result.show(5)\n",
    "\n",
    "# Средний возраст выживших по классам\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Pclass, \n",
    "        AVG(Age) as avg_age,\n",
    "        COUNT(*) as count\n",
    "    FROM titanic \n",
    "    WHERE Survived = 1\n",
    "    GROUP BY Pclass\n",
    "    ORDER BY avg_age\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2ffbf-03ed-4307-87c3-afa55820a722",
   "metadata": {},
   "source": [
    "Комбинация SQL и DataFrame API. Фильтрация через SQL, обработка через DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "868f6c0d-a102-4d3f-b41e-037ef70085ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Pclass|         avg(Fare)|\n",
      "+------+------------------+\n",
      "|     1| 70.36486220472443|\n",
      "|     3| 14.64408300283288|\n",
      "|     2|20.327439024390245|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Фильтруем данные через SQL\n",
    "filtered_df = spark.sql(\"SELECT * FROM titanic WHERE Embarked = 'S'\")\n",
    "\n",
    "# Агрегация через DataFrame API\n",
    "from pyspark.sql.functions import avg\n",
    "filtered_df.groupBy(\"Pclass\").agg(avg(\"Fare\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba302c2f-f3f6-45cf-b25f-48f52bf25bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|age_group|count(1)|\n",
      "+---------+--------+\n",
      "|    Adult|     778|\n",
      "|    Child|     113|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Добавляем столбец через SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, \n",
    "        CASE \n",
    "            WHEN Age < 18 THEN 'Child' \n",
    "            ELSE 'Adult' \n",
    "        END AS age_group\n",
    "    FROM titanic\n",
    "\"\"\").createOrReplaceTempView(\"titanic_with_age_group\")\n",
    "\n",
    "# Используем новое представление\n",
    "spark.sql(\"SELECT age_group, COUNT(*) FROM titanic_with_age_group GROUP BY age_group\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e795aea-b058-4eda-9395-93f86983d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+---------+\n",
      "|                Name|    Fare|Pclass|fare_rank|\n",
      "+--------------------+--------+------+---------+\n",
      "|    Ward, Miss. Anna|512.3292|     1|        1|\n",
      "|Cardeza, Mr. Thom...|512.3292|     1|        1|\n",
      "|Lesurer, Mr. Gust...|512.3292|     1|        1|\n",
      "|Fortune, Mr. Char...|   263.0|     1|        2|\n",
      "|Fortune, Miss. Ma...|   263.0|     1|        2|\n",
      "|Fortune, Miss. Al...|   263.0|     1|        2|\n",
      "|   Fortune, Mr. Mark|   263.0|     1|        2|\n",
      "|Ryerson, Miss. Em...| 262.375|     1|        3|\n",
      "|\"Ryerson, Miss. S...| 262.375|     1|        3|\n",
      "|Baxter, Mr. Quigg...|247.5208|     1|        4|\n",
      "+--------------------+--------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Оконные функции в SQL.\n",
    "# Ранжирование пассажиров по стоимости билета\n",
    "query = \"\"\"\n",
    "    SELECT Name, Fare, Pclass, DENSE_RANK() OVER (PARTITION BY Pclass ORDER BY Fare DESC) as fare_rank\n",
    "    FROM titanic\n",
    "    WHERE Fare IS NOT NULL\n",
    "\"\"\"\n",
    "spark.sql(query).show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
