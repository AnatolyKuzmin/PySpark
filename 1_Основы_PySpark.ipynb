{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f144fb57-b165-4b55-b118-91fbf186dbca",
   "metadata": {},
   "source": [
    "# 1. Основы PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6fd6e-4316-488a-b399-e072717460ad",
   "metadata": {},
   "source": [
    "**Всегда закрывайте сессию после работы**\n",
    "```\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd723522-9d3e-4f8e-b099-5656a15b755a",
   "metadata": {},
   "source": [
    "Установите Java (обязательно!). Скачайте Amazon Corretto 11 (или OpenJDK 11).  \n",
    "```\n",
    "# Проверка (в PowerShell):\n",
    "java -version\n",
    "```\n",
    "Установите Python 3.11. Скачайте с официального сайта. При установке отметьте \"Add Python to PATH\".  \n",
    "```\n",
    "python --version\n",
    "pip --version\n",
    "```\n",
    "Установите Hadoop и Spark. Скачайте Spark 3.5.x (предпочтительно) и Hadoop 3.3. Распакуйте архив в C:\\spark (или другой путь без пробелов). Скачайте winutils.exe для Hadoop в папку C:\\hadoop\\bin. Добавьте переменные среды:\n",
    "```\n",
    "[System.Environment]::SetEnvironmentVariable(\"HADOOP_HOME\", \"C:\\hadoop\", \"Machine\")\n",
    "[System.Environment]::SetEnvironmentVariable(\"SPARK_HOME\", \"C:\\spark\", \"Machine\")\n",
    "[System.Environment]::SetEnvironmentVariable(\"PATH\", \"$env:PATH;C:\\spark\\bin;C:\\hadoop\\bin\", \"Machine\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61767cb0-7665-4058-8690-950b29c46303",
   "metadata": {},
   "source": [
    "Установите PySpark и зависимости\n",
    "```\n",
    "pip install pyspark pandas jupyter pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61879ce-7486-4c23-9077-1981728a1780",
   "metadata": {},
   "source": [
    "Проверка установки. Тест PySpark в Jupyter\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "В ноутбуке выполните\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()\n",
    "```\n",
    "Вывод\n",
    "\n",
    "| id| name|\n",
    "| -- | -- |\n",
    "|  1|Alice|\n",
    "|  2|  Bob|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2723727-466e-4578-83b5-e9ac4066fd38",
   "metadata": {},
   "source": [
    "**Используемые датасеты**  \n",
    "[Titanic](https://www.kaggle.com/competitions/titanic/data?select=train.csv \"CSV\")  \n",
    "[Amazon Sales Data](https://www.kaggle.com/datasets/karkavelrajaj/amazon-sales-dataset \"JSON\")  \n",
    "[COVID-19 Dataset](https://www.kaggle.com/datasets/imdevskp/corona-virus-report \"Parquet\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141d635f-e7e6-4335-96af-c11172ee1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8e0e10-f255-4d5d-b951-65497271a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'  # Обход проверки пользователя\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "    df.show()\n",
    "finally:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d4d85-606d-4747-8eb4-b0c3357cdd7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1. Введение в Spark и RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66d445-4c6d-46b6-89c4-38e8e4f16c4c",
   "metadata": {},
   "source": [
    "PySpark — это Python API для Apache Spark — высокопроизводительной распределённой платформы для обработки больших данных. Позволяет писать параллельные программы, которые работают на кластерах, но при этом использовать знакомый Python. Spark работает с большими объёмами данных, поддерживает SQL, машинное обучение, потоковую обработку. Jupyter Notebook — удобный инструмент для интерактивного написания и запуска кода, идеально подходит для обучения и прототипирования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9718d-52f5-4478-acd8-7446dc4eb836",
   "metadata": {},
   "source": [
    "RDD(Resilient Distributed Dataset) - базовая структура данных в Spark. **Неизменяемая:** После создания нельзя изменить, только преобразовать. **Распределённая:** Данные разделены на части (партиции) и обрабатываются параллельно.  \n",
    "SparkSession – точка входа для работы с Spark (аналог SparkContext в старых версиях)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a75df-0bfc-4b26-a398-c7be755904d2",
   "metadata": {},
   "source": [
    "Ключевые характеристики RDD:\n",
    "- Resilient (Устойчивость). RDD автоматически восстанавливается при сбоях благодаря lineage (цепочке преобразований). Если часть данных теряется, Spark пересчитывает её заново на основе исходных данных и применённых операций.\n",
    "- Distributed (Распределённость). Данные разбиваются на партиции (partitions) и распределяются по узлам кластера. Обработка происходит параллельно.\n",
    "- Dataset (Набор данных). Может содержать данные любого типа (числа, строки, объекты и т. д.). Поддерживает операции: map, filter, reduce, join и др.\n",
    "\n",
    "Как создаются RDD?  \n",
    "```\n",
    "# Из внешних данных (HDFS, S3, локальная файловая система):\n",
    "rdd = sc.textFile(\"hdfs://path/to/file.txt\")\n",
    "\n",
    "# Из коллекций в памяти (например, список Python):\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Преобразованием другого RDD (например, map, filter):\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b567348-c843-4ef0-8edd-0fab93c5767f",
   "metadata": {},
   "source": [
    "### Пример:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b059f-9ea1-44e5-acfa-4c5febe10f60",
   "metadata": {},
   "source": [
    "Создание RDD из списка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f75fe2-a788-4e66-a557-781229d7604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD Basics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Создание RDD из списка чисел\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Трансформация: умножение каждого элемента на 2\n",
    "rdd_doubled = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Действие: сбор результатов\n",
    "print(rdd_doubled.collect())  # Вывод: [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb19b1-8477-4899-865e-0c5c1e5f937c",
   "metadata": {},
   "source": [
    "Фильтрация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb674fb2-e085-4cbd-bb42-4ff47db84e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Фильтрация чётных чисел\n",
    "rdd_even = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(rdd_even.collect())  # Вывод: [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d7116-dca7-43e3-9ad8-42d2ecad13a7",
   "metadata": {},
   "source": [
    "Работа с текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39b1715-092b-4109-b5f2-f0545e3877aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Hello': 3, 'Spark': 1, 'Python': 1, 'World': 1})\n"
     ]
    }
   ],
   "source": [
    "text_data = [\"Hello Spark\", \"Hello Python\", \"Hello World\"]\n",
    "text_rdd = spark.sparkContext.parallelize(text_data)\n",
    "\n",
    "# Разбиваем строки на слова\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Подсчёт слов\n",
    "word_counts = words_rdd.countByValue()\n",
    "print(word_counts)  # Вывод: {'Hello': 3, 'Spark': 1, 'Python': 1, 'World': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6135ff-0229-4983-93d4-6b38fd44c3a8",
   "metadata": {},
   "source": [
    "## 1.2. DataFrames (основы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf53a31-8581-4775-b133-d1a676fcba57",
   "metadata": {},
   "source": [
    "PySpark DataFrame - это распределенная коллекция данных, организованная в именованные столбцы, предоставляемая Apache Spark для работы с большими данными в Python. Это аналог pandas DataFrame, но предназначенный для работы в распределенной среде.\n",
    "\n",
    "**Основные характеристики**. *Распределенная структура*: Данные разделены между узлами кластера. *Неизменяемость*: DataFrame иммутабельны, все операции создают новые DataFrame. *Ленивые вычисления*: Операции выполняются только при вызове действия (action). *Оптимизация*: Использует Catalyst Optimizer для оптимизации запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83cbb15-0aff-4d4e-87c6-a3679c7e6b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d6c73-ff55-4ca7-993a-85fe20239ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c1aae-6930-4102-b6f0-ceaf506286c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449f715-2ff0-4db9-a5a3-e8356a465318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae4e68-837d-4728-8fb3-d90af2e90584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df15083e-e2d9-4afc-844c-23229d66cfe1",
   "metadata": {},
   "source": [
    "## 1.3. Базовые операции с DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89093b6a-0637-445b-9453-84d918a3e00c",
   "metadata": {},
   "source": [
    "## 1.4. Работа с дубликатами и пропусками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ca272-bb80-4848-a66a-2adf0eb822fb",
   "metadata": {},
   "source": [
    "## 1.5. SQL-синтаксис в PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c63abf-de87-41ba-9b7f-60a3a66f604c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73267ed9-9e9a-44fb-aa47-d5335a6a2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
