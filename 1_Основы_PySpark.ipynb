{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f144fb57-b165-4b55-b118-91fbf186dbca",
   "metadata": {},
   "source": [
    "# 1. Основы PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6fd6e-4316-488a-b399-e072717460ad",
   "metadata": {},
   "source": [
    "**Всегда закрывайте сессию после работы**\n",
    "```\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd723522-9d3e-4f8e-b099-5656a15b755a",
   "metadata": {},
   "source": [
    "Установите Java (обязательно!). Скачайте Amazon Corretto 11 (или OpenJDK 11).  \n",
    "```\n",
    "# Проверка (в PowerShell):\n",
    "java -version\n",
    "```\n",
    "Установите Python 3.11. Скачайте с официального сайта. При установке отметьте \"Add Python to PATH\".  \n",
    "```\n",
    "python --version\n",
    "pip --version\n",
    "```\n",
    "Установите Hadoop и Spark. Скачайте Spark 3.5.x (предпочтительно) и Hadoop 3.3. Распакуйте архив в C:\\spark (или другой путь без пробелов). Скачайте winutils.exe для Hadoop в папку C:\\hadoop\\bin. Добавьте переменные среды:\n",
    "```\n",
    "[System.Environment]::SetEnvironmentVariable(\"HADOOP_HOME\", \"C:\\hadoop\", \"Machine\")\n",
    "[System.Environment]::SetEnvironmentVariable(\"SPARK_HOME\", \"C:\\spark\", \"Machine\")\n",
    "[System.Environment]::SetEnvironmentVariable(\"PATH\", \"$env:PATH;C:\\spark\\bin;C:\\hadoop\\bin\", \"Machine\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61767cb0-7665-4058-8690-950b29c46303",
   "metadata": {},
   "source": [
    "Установите PySpark и зависимости\n",
    "```\n",
    "pip install pyspark pandas jupyter pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61879ce-7486-4c23-9077-1981728a1780",
   "metadata": {},
   "source": [
    "Проверка установки. Тест PySpark в Jupyter\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "В ноутбуке выполните\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()\n",
    "```\n",
    "Вывод\n",
    "\n",
    "| id| name|\n",
    "| -- | -- |\n",
    "|  1|Alice|\n",
    "|  2|  Bob|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2723727-466e-4578-83b5-e9ac4066fd38",
   "metadata": {},
   "source": [
    "**Используемые датасеты**  \n",
    "[Titanic](https://www.kaggle.com/competitions/titanic/data?select=train.csv \"CSV\")  \n",
    "[Amazon Sales Data](https://www.kaggle.com/datasets/karkavelrajaj/amazon-sales-dataset \"JSON\")  \n",
    "[COVID-19 Dataset](https://www.kaggle.com/datasets/imdevskp/corona-virus-report \"Parquet\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141d635f-e7e6-4335-96af-c11172ee1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8e0e10-f255-4d5d-b951-65497271a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['HADOOP_USER_NAME'] = 'root'  # Обход проверки пользователя\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "    df.show()\n",
    "finally:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d4d85-606d-4747-8eb4-b0c3357cdd7d",
   "metadata": {},
   "source": [
    "## 1.1. Введение в Spark и RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66d445-4c6d-46b6-89c4-38e8e4f16c4c",
   "metadata": {},
   "source": [
    "PySpark — это Python API для Apache Spark — высокопроизводительной распределённой платформы для обработки больших данных. Позволяет писать параллельные программы, которые работают на кластерах, но при этом использовать знакомый Python. Spark работает с большими объёмами данных, поддерживает SQL, машинное обучение, потоковую обработку. Jupyter Notebook — удобный инструмент для интерактивного написания и запуска кода, идеально подходит для обучения и прототипирования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9718d-52f5-4478-acd8-7446dc4eb836",
   "metadata": {},
   "source": [
    "RDD(Resilient Distributed Dataset) - базовая структура данных в Spark. **Неизменяемая:** После создания нельзя изменить, только преобразовать. **Распределённая:** Данные разделены на части (партиции) и обрабатываются параллельно.  \n",
    "SparkSession – точка входа для работы с Spark (аналог SparkContext в старых версиях)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a75df-0bfc-4b26-a398-c7be755904d2",
   "metadata": {},
   "source": [
    "Ключевые характеристики RDD:\n",
    "- Resilient (Устойчивость). RDD автоматически восстанавливается при сбоях благодаря lineage (цепочке преобразований). Если часть данных теряется, Spark пересчитывает её заново на основе исходных данных и применённых операций.\n",
    "- Distributed (Распределённость). Данные разбиваются на партиции (partitions) и распределяются по узлам кластера. Обработка происходит параллельно.\n",
    "- Dataset (Набор данных). Может содержать данные любого типа (числа, строки, объекты и т. д.). Поддерживает операции: map, filter, reduce, join и др.\n",
    "\n",
    "Как создаются RDD?  \n",
    "```\n",
    "# Из внешних данных (HDFS, S3, локальная файловая система):\n",
    "rdd = sc.textFile(\"hdfs://path/to/file.txt\")\n",
    "\n",
    "# Из коллекций в памяти (например, список Python):\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Преобразованием другого RDD (например, map, filter):\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b567348-c843-4ef0-8edd-0fab93c5767f",
   "metadata": {},
   "source": [
    "### Пример:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b059f-9ea1-44e5-acfa-4c5febe10f60",
   "metadata": {},
   "source": [
    "Создание RDD из списка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f75fe2-a788-4e66-a557-781229d7604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD Basics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Создание RDD из списка чисел\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Трансформация: умножение каждого элемента на 2\n",
    "rdd_doubled = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Действие: сбор результатов\n",
    "print(rdd_doubled.collect())  # Вывод: [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb19b1-8477-4899-865e-0c5c1e5f937c",
   "metadata": {},
   "source": [
    "Фильтрация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb674fb2-e085-4cbd-bb42-4ff47db84e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Фильтрация чётных чисел\n",
    "rdd_even = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(rdd_even.collect())  # Вывод: [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d7116-dca7-43e3-9ad8-42d2ecad13a7",
   "metadata": {},
   "source": [
    "Работа с текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39b1715-092b-4109-b5f2-f0545e3877aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Hello': 3, 'Spark': 1, 'Python': 1, 'World': 1})\n"
     ]
    }
   ],
   "source": [
    "text_data = [\"Hello Spark\", \"Hello Python\", \"Hello World\"]\n",
    "text_rdd = spark.sparkContext.parallelize(text_data)\n",
    "\n",
    "# Разбиваем строки на слова\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Подсчёт слов\n",
    "word_counts = words_rdd.countByValue()\n",
    "print(word_counts)  # Вывод: {'Hello': 3, 'Spark': 1, 'Python': 1, 'World': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6135ff-0229-4983-93d4-6b38fd44c3a8",
   "metadata": {},
   "source": [
    "## 1.2. DataFrames (основы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf53a31-8581-4775-b133-d1a676fcba57",
   "metadata": {},
   "source": [
    "PySpark DataFrame - это распределенная коллекция данных, организованная в именованные столбцы, предоставляемая Apache Spark для работы с большими данными в Python. Это аналог pandas DataFrame, но предназначенный для работы в распределенной среде.\n",
    "\n",
    "**Основные характеристики**. *Распределенная структура*: Данные разделены между узлами кластера. *Неизменяемость*: DataFrame иммутабельны, все операции создают новые DataFrame. *Ленивые вычисления*: Операции выполняются только при вызове действия (action). *Оптимизация*: Использует Catalyst Optimizer для оптимизации запросов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cee046-d0bf-4a78-ae97-160361fbc7db",
   "metadata": {},
   "source": [
    "Отличие от RDD:\n",
    "- Оптимизирован для SQL-подобных операций.\n",
    "- Имеет схему (типы данных колонок).\n",
    "\n",
    "Основные методы:\n",
    "- show() – вывод данных.\n",
    "- printSchema() – вывод структуры.\n",
    "- select(), filter(), groupBy(), agg()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e0e3f-cdf8-4b21-a1ad-738f56cf9e0b",
   "metadata": {},
   "source": [
    "### Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921c603-628a-4cc6-af57-f08abcd36bc5",
   "metadata": {},
   "source": [
    "Воспользуемся датасет Titanic (файл train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0c1aae-6930-4102-b6f0-ceaf506286c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Загрузка CSV с заголовком и выводом схемы\n",
    "df = spark.read.csv(\"data/train.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbbae4-d6fa-4447-be7f-1741ae08d0a7",
   "metadata": {},
   "source": [
    "Исследование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffae4e68-837d-4728-8fb3-d90af2e90584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------------+-----------------+\n",
      "|summary|               Age|             Fare|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|               714|              891|\n",
      "|   mean| 29.69911764705882| 32.2042079685746|\n",
      "| stddev|14.526497332334035|49.69342859718089|\n",
      "|    min|              0.42|              0.0|\n",
      "|    max|              80.0|         512.3292|\n",
      "+-------+------------------+-----------------+\n",
      "\n",
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Показать первые 5 строк\n",
    "df.show(5)\n",
    "\n",
    "# Статистика по числовым колонкам\n",
    "df.describe([\"Age\", \"Fare\"]).show()\n",
    "\n",
    "# Количество выживших и погибших\n",
    "df.groupBy(\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0b766-2361-4724-aedb-8886fcc89595",
   "metadata": {},
   "source": [
    "Фильтрация и агрегация. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6dea02c-afc4-4205-8ccb-2c893a0ba675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------+\n",
      "|Pclass|           AvgFare| MaxFare|\n",
      "+------+------------------+--------+\n",
      "|     1| 84.15468749999992|512.3292|\n",
      "|     3|13.675550101832997|   69.55|\n",
      "|     2| 20.66218315217391|    73.5|\n",
      "+------+------------------+--------+\n",
      "\n",
      "+-----------------+\n",
      "|         avg(Age)|\n",
      "+-----------------+\n",
      "|28.84771573604061|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, max  # Импортируем нужные функции\n",
    "\n",
    "# Стоимость билета по классам\n",
    "df.groupBy(\"Pclass\").agg(avg(\"Fare\").alias(\"AvgFare\"), max(\"Fare\").alias(\"MaxFare\")).show()\n",
    "\n",
    "# Средний возраст выживших женщин.\n",
    "df.filter((df.Sex == \"female\") & (df.Survived == 1)).select(avg(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7689915f-5e50-4ecc-900f-abaf40bcdd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Pclass|count|\n",
      "+------+-----+\n",
      "|     1|  216|\n",
      "|     3|  491|\n",
      "|     2|  184|\n",
      "+------+-----+\n",
      "\n",
      "+-------+-------+\n",
      "|min_age|min_age|\n",
      "+-------+-------+\n",
      "|   0.42|   80.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min  # Импортируем нужные функции\n",
    "\n",
    "# количество пассажиров в каждом классе (Pclass).\n",
    "df.groupBy(\"Pclass\").count().show()\n",
    "\n",
    "# минимальный и максимальный возраст пассажиров\n",
    "df.agg(min(\"Age\").alias(\"min_age\"), max(\"Age\").alias(\"min_age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be74fe-0b8d-495e-8412-36bda0734ae5",
   "metadata": {},
   "source": [
    "###  JSON в PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95f7ff-e6ad-4299-9846-869b69d8d7a0",
   "metadata": {},
   "source": [
    "JSON – полуструктурированный формат, поддерживающий вложенные поля. Spark автоматически определяет схему, но для сложных структур её лучше задавать вручную. Вложенные поля обрабатываются через . (например, `user.address.city`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81c72d-c725-405a-a37c-597f925cf9e6",
   "metadata": {},
   "source": [
    "Функции для работы с JSON:\n",
    "- from_json() - Преобразует JSON-строку в структуру.\n",
    "- to_json() - Преобразует структуру в JSON-строку.\n",
    "- json_tuple() - Извлекает элементы из JSON-строки.\n",
    "- get_json_object() - Извлекает элемент по JSON-пути."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938d6b2-9416-4347-a86b-d947c73c95db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe767e-794e-4ac3-bb6c-4ebddfa98f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53301c54-4bdd-42ee-9070-79be2023fc44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe64ca-290d-44ab-9721-50334a7f7d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df15083e-e2d9-4afc-844c-23229d66cfe1",
   "metadata": {},
   "source": [
    "## 1.3. Базовые операции с DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89093b6a-0637-445b-9453-84d918a3e00c",
   "metadata": {},
   "source": [
    "## 1.4. Работа с дубликатами и пропусками"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ca272-bb80-4848-a66a-2adf0eb822fb",
   "metadata": {},
   "source": [
    "## 1.5. SQL-синтаксис в PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c63abf-de87-41ba-9b7f-60a3a66f604c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73267ed9-9e9a-44fb-aa47-d5335a6a2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
