{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f144fb57-b165-4b55-b118-91fbf186dbca",
   "metadata": {},
   "source": [
    "# 1. Основы PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d4d85-606d-4747-8eb4-b0c3357cdd7d",
   "metadata": {},
   "source": [
    "## 1.1. Установка и архитектура"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec164b36-f0af-4baf-8225-1ae0521329aa",
   "metadata": {},
   "source": [
    "### 1.1.1. Установка PySpark (pip) + Java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b64f1-b2f6-4ed5-b202-19ce578b5aaf",
   "metadata": {},
   "source": [
    "Создание виртуального окружения (Python 3.11 гарантирует полную совместимость с PySpark без дополнительных костылей)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82f547-63c4-4806-99ef-04ed24a0c18e",
   "metadata": {},
   "source": [
    "Установите Python 3.11 если еще нет:  \n",
    "https://www.python.org/downloads/release/python-3119/  \n",
    "\n",
    "*powershell*  \n",
    "`py -3.11 -m venv pyspark_env`  \n",
    "`pyspark_env\\Scripts\\activate`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719346ff-25a5-439e-9552-0a87d2914e6c",
   "metadata": {},
   "source": [
    "Установка PySpark и зависимостей\n",
    "\n",
    "*powershell*  \n",
    "`pip install pyspark==3.5.0 jupyter pandas pyarrow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ebf845-4b0b-4f62-93b2-1f4dfed428ad",
   "metadata": {},
   "source": [
    "Проверочный код (в первой ячейке Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f32b5ba-1f00-4caf-8de9-15ee0014c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "VenV: True\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   10|\n",
      "|   20|\n",
      "|   30|\n",
      "|   40|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "\n",
    "# Проверка версий\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"VenV: {sys.prefix != sys.base_prefix}\")  # Проверим, что в VENV\n",
    "\n",
    "# Инициализация Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark 3.11 Test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Тестовый расчет\n",
    "df = spark.range(1, 5)\n",
    "df.selectExpr(\"id * 10 as value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22a770-bbe4-4378-9512-72bd4e03e980",
   "metadata": {},
   "source": [
    "1. Создайте DataFrame с колонками:\n",
    "- user_id (integer)\n",
    "- action (string: \"login\", \"purchase\", \"logout\")\n",
    "- timestamp (current time)\n",
    "\n",
    "2. Используйте withColumn для добавления колонки is_purchase (boolean)\n",
    "\n",
    "3. Отфильтруйте только события покупки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d565192-c8c3-4a29-83a8-7985ffe2c491",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`action`, `user_id`, `timestamp`].; line 1 pos 0;\n'Project [('id * 10) AS value#73]\n+- LogicalRDD [user_id#67L, action#68, timestamp#69], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m data = [\n\u001b[32m      6\u001b[39m     (\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlogin\u001b[39m\u001b[33m\"\u001b[39m, datetime.now()),\n\u001b[32m      7\u001b[39m     (\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpurchase\u001b[39m\u001b[33m\"\u001b[39m, datetime.now()),\n\u001b[32m      8\u001b[39m     (\u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpurchase\u001b[39m\u001b[33m\"\u001b[39m, datetime.now()),\n\u001b[32m      9\u001b[39m     (\u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlogout\u001b[39m\u001b[33m\"\u001b[39m, datetime.now())\n\u001b[32m     10\u001b[39m ]\n\u001b[32m     12\u001b[39m df = spark.createDataFrame(data, [\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid * 10 as value\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Добавляем колонку is_purchase\u001b[39;00m\n\u001b[32m     16\u001b[39m df = df.withColumn(\u001b[33m\"\u001b[39m\u001b[33mis_purchase\u001b[39m\u001b[33m\"\u001b[39m, F.col(\u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mpurchase\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PySpark\\pyspark_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3263\u001b[39m, in \u001b[36mDataFrame.selectExpr\u001b[39m\u001b[34m(self, *expr)\u001b[39m\n\u001b[32m   3261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m   3262\u001b[39m     expr = expr[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PySpark\\pyspark_env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PySpark\\pyspark_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`action`, `user_id`, `timestamp`].; line 1 pos 0;\n'Project [('id * 10) AS value#73]\n+- LogicalRDD [user_id#67L, action#68, timestamp#69], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# Создаем тестовые данные\n",
    "data = [\n",
    "    (1, \"login\", datetime.now()),\n",
    "    (1, \"purchase\", datetime.now()),\n",
    "    (2, \"purchase\", datetime.now()),\n",
    "    (2, \"logout\", datetime.now())\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"action\", \"timestamp\"])\n",
    "\n",
    "# Добавляем колонку is_purchase\n",
    "df = df.withColumn(\"is_purchase\", F.col(\"action\") == \"purchase\")\n",
    "\n",
    "# Фильтрация\n",
    "purchases = df.filter(F.col(\"is_purchase\"))\n",
    "\n",
    "# Результат\n",
    "print(\"Все события:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\nТолько покупки:\")\n",
    "display(purchases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9349ad2-ce7a-4631-8700-ee6bc2fdcdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "Java Home: C:\\Program Files\\Eclips_Adoptium\\jdk-11.0.27.6-hotspot\n",
      "Spark Version: 3.5.0\n",
      "Master: local[*]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Java Home: {os.getenv('JAVA_HOME')}\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(f\"Spark Version: {sc.version}\")\n",
    "print(f\"Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09790de-2747-46db-9275-5fd65438c136",
   "metadata": {},
   "source": [
    "### 1.1.2. Настройка Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b9841-061b-4be6-b0b5-032867278f5a",
   "metadata": {},
   "source": [
    "### 1.1.3. Архитектура Spark (Driver, Executor, Cluster Manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47b60a-948f-4040-8611-440cc57faed9",
   "metadata": {},
   "source": [
    "### 1.1.4. RDD vs DataFrame vs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7c24e-58c5-4d20-86fd-710c8c86c619",
   "metadata": {},
   "source": [
    "### 1.1.5. Практика: Создание SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c340d-0f23-42a7-bfa9-f81ee596c950",
   "metadata": {},
   "source": [
    "## 1.2. Работа с DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66d445-4c6d-46b6-89c4-38e8e4f16c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
